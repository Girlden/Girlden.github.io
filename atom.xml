<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Girlden&#39;s Blog</title>
  
  <subtitle>Quick notes</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-01-29T06:21:39.215Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Girlden</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>module安装问题</title>
    <link href="http://yoursite.com/2018/01/29/module%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/01/29/module安装问题/</id>
    <published>2018-01-29T06:00:39.000Z</published>
    <updated>2018-01-29T06:21:39.215Z</updated>
    
    <content type="html"><![CDATA[<p>前言：由于我为了方便，开始就安装了Anaconda，所以很多模块都已经装好了，遇到没有的模块，我们可以用pip install +模块名字，但是有的时候也会出现问题。</p><h1 id="wordcloud安装出错"><a href="#wordcloud安装出错" class="headerlink" title="wordcloud安装出错"></a>wordcloud安装出错</h1><p>你会发现在cmd的输入命令 <code>pip install wordcloud</code> 时会报错。<br><strong>解决方法</strong>：</p><p>在以下这个网址<br><a href="http://link.zhihu.com/?target=http%3A//www.lfd.uci.edu/%7Egohlke/pythonlibs/%23lxml" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//www.lfd.uci.edu/%7Egohlke/pythonlibs/%23lxml</a><br>下载你需要的的对应版本模块。</p><p><img src="https://i.imgur.com/WIuFEGl.png" alt=""></p><p>接着，把下载下来的文件原封不动的拷贝到你pyhon或者Anaconda的Scripts中，我的就是F:\Anaconda\Scripts</p><p>再到cmd中执行pip install F:\Anaconda\Scripts\刚才的文件名</p><p>这样就能下载成功！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前言：由于我为了方便，开始就安装了Anaconda，所以很多模块都已经装好了，遇到没有的模块，我们可以用pip install +模块名字，但是有的时候也会出现问题。&lt;/p&gt;
&lt;h1 id=&quot;wordcloud安装出错&quot;&gt;&lt;a href=&quot;#wordcloud安装出错&quot; c
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="模块安装" scheme="http://yoursite.com/tags/%E6%A8%A1%E5%9D%97%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫（3）</title>
    <link href="http://yoursite.com/2018/01/26/python%E7%88%AC%E8%99%AB%EF%BC%883%EF%BC%89/"/>
    <id>http://yoursite.com/2018/01/26/python爬虫（3）/</id>
    <published>2018-01-26T14:25:49.000Z</published>
    <updated>2018-01-26T14:57:49.291Z</updated>
    
    <content type="html"><![CDATA[<p>之前的两节，我们把我们要的东西爬取下来，但是并没有做存储，下面我们将会讲解如何做存储。</p><h2 id="保存到Excel"><a href="#保存到Excel" class="headerlink" title="保存到Excel"></a>保存到Excel</h2><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">df = pandas.DataFrame(<span class="keyword">final</span>)</span><br><span class="line">df.to_excel(<span class="string">'news.xlsx'</span>)</span><br></pre></td></tr></table></figure><p>这是就可以在我们的目录当中找到news.xlsx的文件。</p><h2 id="保存到数据库"><a href="#保存到数据库" class="headerlink" title="保存到数据库"></a>保存到数据库</h2><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="keyword">with</span> sqlite3.connect(<span class="string">'news.sqlite'</span>) <span class="keyword">as</span> db:</span><br><span class="line">    df.to_sql(<span class="string">'news'</span>,con = db)</span><br></pre></td></tr></table></figure><p>可以利用pandas去sqlite中拿数据：<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> sqlite3.connect(<span class="symbol">'news</span>.sqlite') <span class="keyword">as</span> db:</span><br><span class="line">    df2 = pandas.read_sql(<span class="symbol">'SELECT</span> * <span class="type">FROM</span> news',con = db)</span><br></pre></td></tr></table></figure></p><p>连上钱两次，就算是一个完整的爬虫过程了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前的两节，我们把我们要的东西爬取下来，但是并没有做存储，下面我们将会讲解如何做存储。&lt;/p&gt;
&lt;h2 id=&quot;保存到Excel&quot;&gt;&lt;a href=&quot;#保存到Excel&quot; class=&quot;headerlink&quot; title=&quot;保存到Excel&quot;&gt;&lt;/a&gt;保存到Excel&lt;/h
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫（2）</title>
    <link href="http://yoursite.com/2018/01/26/%E7%88%AC%E8%99%AB%EF%BC%882%EF%BC%89/"/>
    <id>http://yoursite.com/2018/01/26/爬虫（2）/</id>
    <published>2018-01-26T03:48:14.000Z</published>
    <updated>2018-01-26T14:16:19.019Z</updated>
    
    <content type="html"><![CDATA[<p>事前的章节只是负责把新闻的网页爬取下来，对于内文的需求还是不够的，所以这篇文章主要讲的是对于内文信息的爬取，包括<strong>标题、时间、作者、来源</strong>等等。</p><p>还和之前的爬去方法一样，利用<strong>Chrome F12</strong>就可以找到网页和获取响应的方法，之前有讲过。代码如下：<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">res = requests.get(<span class="string">"http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml"</span>)</span><br><span class="line">res.encoding = <span class="string">'utf-8'</span></span><br><span class="line">soup = BeautifulSoup(res.<span class="keyword">text</span>,<span class="string">'html.parser'</span>)</span><br></pre></td></tr></table></figure></p><p>这样就获取了我们所需要的网页部分，继续进行标签的搜寻。</p><ol><li>标题。在class=“main-title”的标签之中。</li><li>时间。在class=“date-source”中，但是都在span标签下，可以用contents将不同分层的标签下的东西取出。最后用datetime模块将<strong>时间字符串</strong>变成<strong>时间格式</strong>。</li><li>来源。和上面类似的操作。</li><li>内容。找到后会有分段的内容，利用<strong>for循环</strong>嵌套后就可以取出来，再用<strong>join</strong>连接成一个完整的部分。</li><li>评论数。你在doc中找不到，它在js中，而且还需要用json模块将数据变成我们容易处理的字典。<br>具体代码如下：<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">title = soup.select(<span class="string">'.main-title'</span>)[<span class="number">0</span>].text</span><br><span class="line">date = soup.select(<span class="string">'.date-source'</span>)[<span class="number">0</span>]<span class="selector-class">.contents</span>[<span class="number">1</span>].text</span><br><span class="line"><span class="selector-tag">dt</span> = datetime.strptime(date,<span class="string">'%Y年%m月%d日 %H:%M'</span>)</span><br><span class="line">source = soup.select(<span class="string">'.date-source a '</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="selector-tag">body</span> = soup.select(<span class="string">'.article p'</span>)[:-<span class="number">1</span>]</span><br><span class="line"><span class="selector-tag">article</span> = <span class="string">'\n'</span>.join([d<span class="selector-class">.text</span><span class="selector-class">.strip</span>() <span class="keyword">for</span> d <span class="keyword">in</span> body])</span><br><span class="line">mport json</span><br><span class="line">comment = requests.get(<span class="string">'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\</span></span><br><span class="line"><span class="string">channel=gn&amp;newsid=comos-fyqyesy2133291&amp;\</span></span><br><span class="line"><span class="string">group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\</span></span><br><span class="line"><span class="string">h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495'</span>)</span><br><span class="line">jd = json.loads(comment<span class="selector-class">.text</span><span class="selector-class">.lstrip</span>(<span class="string">'jsonp_1516947697495('</span>).rstrip(<span class="string">')'</span>))</span><br><span class="line">jd[<span class="string">'result'</span>][<span class="string">'count'</span>][<span class="string">'total'</span>]</span><br></pre></td></tr></table></figure></li></ol><p>评论数的获取页面链接中包含新闻<strong>id</strong>，在新闻本身的页面链接能找到。将它独立取出的方法有两个：</p><ol><li><p>用<strong>split</strong>将链接变成多个块组成的list，然后再用索引取，最后用<strong>strip</strong>削掉我们不要的部分。</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span><span class="type">url</span> = <span class="string">'http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml'</span></span><br><span class="line"><span class="keyword">new</span><span class="type">sid</span> = <span class="keyword">new</span><span class="type">url</span>.split(<span class="string">'/'</span>)[<span class="number">-1</span>].lstrip(<span class="string">'doc-i'</span>).rstrip(<span class="string">'.shtml'</span>)</span><br></pre></td></tr></table></figure></li><li><p>利用正则表达去匹配。</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line">m = re.search(<span class="string">'doc-i(.*).shtml'</span>,<span class="keyword">new</span><span class="type">url</span>)</span><br><span class="line">m.group(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><p>当然这是一个页面的评论数，写一个函数就能解决。利用<strong>字符串的format</strong>把上面获得的字符串填入大家共同的部分中。<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">commenturl = <span class="string">'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\</span></span><br><span class="line"><span class="string">channel=gn&amp;newsid=comos-&#123;&#125;&amp;\</span></span><br><span class="line"><span class="string">group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\</span></span><br><span class="line"><span class="string">h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495'</span></span><br><span class="line">def Getcount(<span class="keyword">new</span><span class="type">url</span>):<span class="type"></span></span><br><span class="line"><span class="type">    m </span>= re.search(<span class="string">'doc-i(.*).shtml'</span>,<span class="keyword">new</span><span class="type">url</span>)</span><br><span class="line">    <span class="keyword">new</span><span class="type">sid</span> = m.group(<span class="number">1</span>)</span><br><span class="line">    comment = requests.<span class="keyword">get</span>(commenturl.format(<span class="keyword">new</span><span class="type">sid</span>))</span><br><span class="line">    jd = json.loads(comment.text.lstrip(<span class="string">'jsonp_1516947697495('</span>).rstrip(<span class="string">')'</span>))</span><br><span class="line">    <span class="keyword">return</span> jd[<span class="string">'result'</span>][<span class="string">'count'</span>][<span class="string">'total'</span>]</span><br></pre></td></tr></table></figure></p><p>将上面汇总成一个函数中，通过字典返回我们要的所有内文细节：<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">def News(newurl):</span><br><span class="line">    article = &#123;&#125;</span><br><span class="line">    res = requests.get(newurl)</span><br><span class="line">    res.encoding = <span class="string">'utf-8'</span></span><br><span class="line">    soup = BeautifulSoup(res.<span class="keyword">text</span>,<span class="string">'html.parser'</span>)</span><br><span class="line">    article[<span class="string">'title'</span>] = soup.select(<span class="string">'.main-title'</span>)[<span class="number">0</span>].<span class="keyword">text</span></span><br><span class="line">    date = soup.select(<span class="string">'.date-source'</span>)[<span class="number">0</span>].<span class="built_in">contents</span>[<span class="number">1</span>].<span class="keyword">text</span></span><br><span class="line">    article[<span class="string">'dt'</span>] = datetime.strptime(date,<span class="string">'%Y年%m月%d日 %H:%M'</span>)</span><br><span class="line">    article[<span class="string">'source'</span>] = soup.select(<span class="string">'.date-source a '</span>)[<span class="number">0</span>].<span class="keyword">text</span></span><br><span class="line">    body = soup.select(<span class="string">'.article p'</span>)[:-<span class="number">1</span>]</span><br><span class="line">    article[<span class="string">'articleBody'</span>] = <span class="string">'\n'</span>.join([d.<span class="keyword">text</span>.strip() <span class="keyword">for</span> d <span class="built_in">in</span> body])</span><br><span class="line">    article[<span class="string">'author'</span>] = soup.select(<span class="string">'.show_author'</span>)[<span class="number">0</span>].<span class="keyword">text</span>.lstrip(<span class="string">'责任编辑：'</span>)</span><br><span class="line">    article[<span class="string">'commentcount'</span>] = Getcount(newurl)</span><br><span class="line">    return article</span><br></pre></td></tr></table></figure></p><p>这里定义了一个网页链可以得到的所有内容信息，接下来就是考虑一个页面上的所有网页链接都爬下来。主要是通过在<strong>js中找到不同步的分页信息</strong>才得以了解所有的网页链接是什么。代码如下：<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def parserlist(url): <span class="type"></span></span><br><span class="line"><span class="type">    newdetails </span>= []</span><br><span class="line">    res = requests.<span class="keyword">get</span>(url)</span><br><span class="line">    <span class="keyword">new</span><span class="type">urlnot</span> = res.text.lstrip(<span class="string">'  newsloadercallback('</span>).rstrip(<span class="string">');'</span>)</span><br><span class="line">    jd = json.loads(<span class="keyword">new</span><span class="type">urlnot</span>)</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> jd[<span class="string">'result'</span>][<span class="string">'data'</span>]:<span class="type"></span></span><br><span class="line"><span class="type">        newdetails</span>.append(News(d[<span class="string">'url'</span>]))</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span><span class="type">details</span></span><br></pre></td></tr></table></figure></p><p><strong>parserlist</strong>函数唯一的输入参数就是那个<strong>js</strong>中的网页连接（管理着一个页面的的所有新闻链接）</p><p>但是问题又来了，我们的分页可不止一个，所以我们继续观察分页的网页链接，发现page={数字}表示页数，那么我们可以写一个函数继续扩大工作量。代码如下：<br><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">url = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&amp;cat_1=gnxw&amp;cat_2==gdxw1|<span class="type">|=gatxw</span>|<span class="type">|=zs</span>-pl|<span class="type">|=mtjj</span>&amp;level==<span class="number">1</span>|<span class="type">|=2</span>&amp;show_ext=<span class="number">1</span>&amp;show_all=<span class="number">1</span>&amp;show_num=<span class="number">22</span>&amp;tag=<span class="number">1</span>&amp;format=json&amp;page=&#123;&#125;&amp;callback=newsloadercallback&amp;<span class="keyword">_</span>=<span class="number">1516960730249</span>'</span><br><span class="line">final = []</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1</span>,<span class="number">10</span>):</span><br><span class="line">    newser = parserlist(url.format(i))</span><br><span class="line">    final.extend(newser)</span><br></pre></td></tr></table></figure></p><p>最终的<strong>final</strong>就是我们要的9个页面的所有新闻内文。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;事前的章节只是负责把新闻的网页爬取下来，对于内文的需求还是不够的，所以这篇文章主要讲的是对于内文信息的爬取，包括&lt;strong&gt;标题、时间、作者、来源&lt;/strong&gt;等等。&lt;/p&gt;
&lt;p&gt;还和之前的爬去方法一样，利用&lt;strong&gt;Chrome F12&lt;/strong&gt;就
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫（1）</title>
    <link href="http://yoursite.com/2018/01/25/python%E7%88%AC%E8%99%AB/"/>
    <id>http://yoursite.com/2018/01/25/python爬虫/</id>
    <published>2018-01-25T04:07:52.000Z</published>
    <updated>2018-01-25T16:51:07.584Z</updated>
    
    <content type="html"><![CDATA[<p>今天来简单的学习一下爬取网页的文档，然后归档。</p><h1 id="第一个简单的爬虫"><a href="#第一个简单的爬虫" class="headerlink" title="第一个简单的爬虫"></a>第一个简单的爬虫</h1><p>利用<strong>chrome F12</strong>的检查功能对网页的回应进行读取，这里以新浪网为例。</p><p><img src="https://i.imgur.com/HIf2TXK.png" alt=""></p><p><img src="https://i.imgur.com/5xFFPul.png" alt=""></p><p>获取到这些信息后，我们就能够写出一下代码：<br><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">new_url = 'http:<span class="comment">//news.sina.com.cn/society/</span></span><br><span class="line">res = requests.<span class="keyword">get</span>(new_url)</span><br><span class="line">res.encoding = 'utf-<span class="number">8</span>'</span><br><span class="line"><span class="built_in">print</span>(res.text)</span><br></pre></td></tr></table></figure></p><p>打印出来后表明没有问题，这是可以用到BeautifulSoup这个模块，讲刚才打印出来的东西丢进去，交给它处理，注意还需标明剖析器是html.parser。</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(res.<span class="keyword">text</span>,<span class="string">'html.parser'</span>)</span><br></pre></td></tr></table></figure><p>我们可以知道，在res里有很多的标签，我们需要的文字藏在固定的标签中，所以我们下面要做的就是取出我们需要的标签，需要用到soup的select方法。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup2 = soup.select(<span class="string">'h2'</span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(soup2)</span></span></span><br></pre></td></tr></table></figure><p>结果如下，是一个列表。<br><img src="https://i.imgur.com/nYTzwP3.png" alt=""></p><p>但是我们要爬取的内容的标签是以不同的id和class来区分的，这里涉及到css的相关内容，我们暂且不谈。只需要知道我们需要通过网页的<strong>检查</strong>了解我们的新闻藏在什么杨种类的标签之下。</p><p><img src="https://i.imgur.com/W7x0Sz2.png" alt=""></p><p>现在可以知道我们的内容在news-item的class之下，所以我们运用select(‘.news-item’)的方法去获取，同样的方法，我们又可以知道，新闻的标题在<strong>h标签下</strong>，新闻的时间在<strong>值time的class之下</strong>，链接在<strong>h2标签的href中</strong>，写出一下代码。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(res<span class="selector-class">.text</span>,<span class="string">'html.parser'</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> soup.select(<span class="string">'.news-item'</span>):</span><br><span class="line">    <span class="keyword">if</span> len(link.select(<span class="string">'h2'</span>)) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="selector-tag">h2</span> = link.select(<span class="string">'h2'</span>)[<span class="number">0</span>].text</span><br><span class="line">        <span class="selector-tag">time</span> = link.select(<span class="string">'.time'</span>)[<span class="number">0</span>].text</span><br><span class="line">        href = link.select(<span class="string">'a'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</span><br><span class="line">        print(<span class="selector-tag">time</span>,<span class="selector-tag">h2</span>,href)</span><br></pre></td></tr></table></figure></p><p><strong>注：text取的是标签的文字内容，而[‘href’]取的是标签内部的值。</strong></p><p>最终结果如下：</p><p><img src="https://i.imgur.com/pjSMwDJ.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天来简单的学习一下爬取网页的文档，然后归档。&lt;/p&gt;
&lt;h1 id=&quot;第一个简单的爬虫&quot;&gt;&lt;a href=&quot;#第一个简单的爬虫&quot; class=&quot;headerlink&quot; title=&quot;第一个简单的爬虫&quot;&gt;&lt;/a&gt;第一个简单的爬虫&lt;/h1&gt;&lt;p&gt;利用&lt;strong&gt;chrom
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>寒假</title>
    <link href="http://yoursite.com/2018/01/24/%E5%AF%92%E5%81%87/"/>
    <id>http://yoursite.com/2018/01/24/寒假/</id>
    <published>2018-01-23T16:23:03.000Z</published>
    <updated>2018-01-25T16:51:21.557Z</updated>
    
    <content type="html"><![CDATA[<p>我不知道为什么要写这个东西，只是闲着无聊，有不想动笔写日记，反正也没有喜欢的本子，就把闲置的博客拿来用用吧，希望未来自己看到这个能够有所感触。</p><p>我在从南京回来的路上听到一则鼓励写作的的录音，说写作并不用很好好的文笔只需要保持一直写就可以了。对于这个观点，我相信，原因很简单，我没有文笔，又喜欢写点东西，决定坚持。</p><p>我现在懂了，不是只有难得的事情，做出来了才伟大，简单的事情坚持一直做也是件很了不起的。突然想起来我的高中化学课的老师曾经和我们讲过某企业老板要求自己的员工在每天某个时间点准时起来做一分钟的高抬腿，而且他们真正的做到了。一件事，做一天你可能回说没什么，做一周也还好，一个月很厉害，一年也还好，三年四年。。。你会怎么觉得，我看到一则介绍爱情的短视频，<strong>一个男生为自己喜欢的女生每天削一个苹果，坚持了2300天</strong>，他们没有吵过架。</p><p>从此刻开始，我要为了自己想过的生活儿努力，坚持写作，每天。就和每天吃饭睡觉一样。寒假放假之前，我加了一个学习理财的群，我发现，他们一只都在教我们理财的重要性，大家还在学的那么起劲，我再管网几天，看看又没有干货，有没有的话就退出吧。今天看完了爬虫的一段教程，我觉讲的很有条理，明天继续。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我不知道为什么要写这个东西，只是闲着无聊，有不想动笔写日记，反正也没有喜欢的本子，就把闲置的博客拿来用用吧，希望未来自己看到这个能够有所感触。&lt;/p&gt;
&lt;p&gt;我在从南京回来的路上听到一则鼓励写作的的录音，说写作并不用很好好的文笔只需要保持一直写就可以了。对于这个观点，我相信
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Datastruct</title>
    <link href="http://yoursite.com/2017/12/12/datastruct/"/>
    <id>http://yoursite.com/2017/12/12/datastruct/</id>
    <published>2017-12-12T02:51:57.000Z</published>
    <updated>2018-01-25T16:50:49.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性表"><a href="#线性表" class="headerlink" title="线性表"></a>线性表</h1><h2 id="顺序表示"><a href="#顺序表示" class="headerlink" title="顺序表示"></a>顺序表示</h2><p><strong>DestroyLis</strong>t是将线性表的所有内容释放。线性表有三个属性，分别是线性表的空间基址、当前内容的长度<strong>length</strong>以及分配的长度<strong>size</strong>。（这里分配的长度需要大于内容的长度）<strong>DestroyList</strong>是讲所有的属性抹去初始值。</p><p><strong>ClearList</strong>是将当前的内容长度置0。</p><p><strong>isEmpty</strong>则是通过内容长度来判断线性表是否为空。</p><p><strong>GetElem</strong>是将第i个元素取出来并返回。</p><p><strong>FindElem</strong>是将表中元素与查找元素相同的下标输出。</p><p>对应的还有插入、删除、访问、历遍等等。。。。。</p><h2 id="链式表示"><a href="#链式表示" class="headerlink" title="链式表示"></a>链式表示</h2><p>链式表示和顺序表示最大的不同就是，链式表示中逻辑位置相邻的数据他们需要物理位置也相邻。插入和删除不需要再将大量的数据做移动，减少算法的复杂度。</p><p>定义单链表时会有两个属性，1.数据，2.下一个节点的指针。</p><p>作业，编写代码<strong>实现链表反转和实现一元多项式的相加</strong>。</p><p>12/12/2017 11:46:54 AM </p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性表&quot;&gt;&lt;a href=&quot;#线性表&quot; class=&quot;headerlink&quot; title=&quot;线性表&quot;&gt;&lt;/a&gt;线性表&lt;/h1&gt;&lt;h2 id=&quot;顺序表示&quot;&gt;&lt;a href=&quot;#顺序表示&quot; class=&quot;headerlink&quot; title=&quot;顺序表示&quot;&gt;&lt;/a&gt;顺序表
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>考研前的课程复习安排</title>
    <link href="http://yoursite.com/2017/11/27/%E8%80%83%E7%A0%94%E5%89%8D%E7%9A%84%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0%E5%AE%89%E6%8E%92/"/>
    <id>http://yoursite.com/2017/11/27/考研前的课程复习安排/</id>
    <published>2017-11-27T11:37:16.000Z</published>
    <updated>2018-01-25T16:51:34.418Z</updated>
    
    <content type="html"><![CDATA[<p>这学期可能是我大学期间的转折点，也将可能会是我大学阶段面对考试最困难的一个学期，因为我大三了，而且面临考验抉择，所以这个学期志在必得（一门都不许挂），下面是我最低限度的复习计划，针对每天整理出最少的复习量。</p><p><img src="https://i.imgur.com/nQ5pPSQ.jpg" alt=""></p><p>这学期的课程分为6门：通原，通线相对可以接受，交换，信息论，单片机，电磁场很难。</p><p>现在是第13周，距离考试还有4周不到的时间，我暂且粗略的估计是20天，还留七八天用来巩固。</p><h1 id="通原"><a href="#通原" class="headerlink" title="通原"></a>通原</h1><p>一共13章，一天一章，从明天开始。</p><h1 id="通线"><a href="#通线" class="headerlink" title="通线"></a>通线</h1><p>一共10章，一天一章，从明天开始。</p><h1 id="电磁场"><a href="#电磁场" class="headerlink" title="电磁场"></a>电磁场</h1><p>一共8章，一天一章，从明天开始。</p><h1 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h1><p>一共11章，一天一章，从12月6日开始。</p><h1 id="单片机"><a href="#单片机" class="headerlink" title="单片机"></a>单片机</h1><p>一共11章，一天一章，从12月7日开始。</p><p>#信息论 #<br>一共7章，一天一章，从12月11日开始。</p><p>这些课程除外还有一节毛概，一节通创，一节就业指导，以及一节实验。</p><p>综合起来将会有10门功课，加油吧，就这么多了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这学期可能是我大学期间的转折点，也将可能会是我大学阶段面对考试最困难的一个学期，因为我大三了，而且面临考验抉择，所以这个学期志在必得（一门都不许挂），下面是我最低限度的复习计划，针对每天整理出最少的复习量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/11/23/hello-world/"/>
    <id>http://yoursite.com/2017/11/23/hello-world/</id>
    <published>2017-11-23T15:47:33.091Z</published>
    <updated>2017-11-23T15:47:33.091Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
