<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[什么是数据化运营]]></title>
    <url>%2F2018%2F03%2F01%2F%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1.1 现代营销理论的发展4P到4C4P指product、Price、Place、Promotion。product：产品注重功能，强调卖点。price：价格需要根据不同的市场定位。place：渠道注重分销商的培养和销售网络的建设。promotion：企业通过改变销售行为来刺激消费，以短期的行为促进消费增长。 4P理论的核心是Product。 随着时代的发展，商品丰富起来，市场竞争也日益激烈。在21实际消费者成为了商业的核心。这时4C理论兴起。4C是指consumer、cost、communication、convenience。consumer：是指消费者的需求和欲望。cost：是指消费者得到满足的成本。communication：是指与用户的沟通。convenience：是指用户购买的方便性。 4C理论的核心是消费者。 4C到3P3C大数据时代的来临，4C理论再次落后于时代发展的需求，海量数据的堆积和存储等迫使现代企业不得不寻找更适合、更可控、更可量化的营销思路和方法论。3P3C的六要素：Probability：营销、运营活动以概率为核心，追求精细化和精准率。Product：产品prospect：目标用户Creative：创新Channel：渠道Cost：成本 数据分析挖掘所支撑的目标响应概率是核心。这里的目标响应概率不应狭义理解为仅仅是预测响应模型之类的响应概率，它有更加宽泛的含义。从宏观上理解，概率可以是特定消费者整体上的概率或可能性。比如，我们用卡方检验某个特定的类别的消费群体在某一个消费行为指标上具有显著性特征。从微观上理解，概率可以具体到某一个特定消费者的“预期响应概率”，比如用逻辑回归算法搭建一预测响应模型，得到每个用户的预计响应概率。 数据化运营的主要内容数据化运营业内没有统一的定义，但是其基本要素和核心是一致的，那就是:“以企业级海量数据的存储和挖掘应用为核心支持，企业全员参与的，以精准、细分和精细化为特点的企业运营制度和战略。” 浅层次的理解的话，就是在企业常规运营的基础上革命性地增添数据分析和挖掘的精准支持。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据化化运营</tag>
        <tag>营销</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据导入]]></title>
    <url>%2F2018%2F02%2F25%2FMySQLL%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[在安装完MySQL数据库后，数据库中的数据呈现最初始的状态，所以需要向其中填充数据，但是这是数据准备阶段的工作。下面简单介绍一下： 构建数据库和table 首先需要进入数据库。在管理员命令窗口输入net start mysql5.7.21。 然后再登陆mysql。 1mysql -u root -p 进入后再创建database。 1CREATE DATABASE test2 #默认是以utf8编码 进入test2这个database中。 1use test2 再进行CSV文件的导入。以table的形式。 123456CREATE TABLE TEST_BOOKS1( team_name varchar(100), Wins int, Draws int, Losses int); 创建table的格式如下：变量名 类型 限制以上的实例就是我的表格结构的概括，可以根据自己的需求自行修改参数： 导入数据导入将自己文件里的数据导入到这个构造好的table中。123456LOAD DATA INFILE 'C:\Users\diannao\Desktop\2015-16-premier-league.csv'INTO TABLE TEST_BOOKS1FIELDS TERMINATED BY ','ENCLOSED BY '"'LINES TERMINATED BY '\n'IGONRE 1 ROWS; 错误解决这里需要提一下，如果运行不成功，并且显示的错误是secure-file-priv有问题，那么就需要在my.ini文件中的[mysqld]下进行声明secure-file-priv=,这里的意思就是对于导入和导出不去进行干扰。注意，声明完成后，还需要在管理员命令窗口下对mysql5.7.21进行重启，才能有效。 检验检验的方法SELECT * FROM TEST_BOOKS1到这里就算导入成功了！]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据准备</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL安装与配置]]></title>
    <url>%2F2018%2F02%2F24%2FMySQL%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[我们首先可以在Mysql的官网上找到我们的需要下载的文件。选择符合自己电脑的版本。 这些都是不需要安装的压缩包，这里就讲一下怎么用这个压缩包安装。 解压文件，在文件里添加初始化文件my.ini。先新建txt文档然后再更名为my.ini。在此文档里添加一段初始化的代码。12345678910111213141516171819[mysql]#设置mysql客户端默认字符集default-character-set=utf8[mysqld]#设置3306端口port=3306#设置mysql的安装目录basedir=D:\Program Files\mysql-5.7.21-winx64#设置mysql数据库的数据库的存放目录datadir=D:\Program Files\mysql-5.7.21-winx64\data#允许最大连接数max_connections = 200#服务端使用的字符集默认为8比特的latin1字符集character-set-server=utf8#开启查询缓存explicit_defaults_for_timestamp = trueskip-grant-tables#创建新表将使用的默认存储引擎default-storage-engine=INNODB 需要注意的是：上面第二个【】里是mysqld。 配置系统变量。在系统变量里添加MYSQL_HOME,路径填你解压文件所在的目录下，我的是D:\Program Files\mysql-5.7.21-winx64。然后在系统path里添加：%MYSQL_HOME%\bin。 以管理员身份运行命令终端cmd。执行一下代码： 123d:cd *D:\Program Files\mysql-5.7.21-winx64\binmysqld --initialize --user=mysql --console 安装服务。 1mysqld --install MySQL5.7.21 启动服务。 1net start MySQL5.7.21 修改密码。 1set password for root@local=password('你的密码') 每次登陆mysql，就输入1mysql -u root -p]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Python进行数据分析之Pandas]]></title>
    <url>%2F2018%2F02%2F21%2F%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPandas%2F</url>
    <content type="text"><![CDATA[2008年开始就有pandas库，构建者构造它的原因就是没有任何一个工具可以满足他的工作需求。 具备按轴自动或显示数据的对齐的功能的数据结构。 集成时间序列功能。 既能处理时间序列数据也能处理非时间序列数据的数据结构。 数据运算和约简可以根据不同的元素数据执行。 灵活处理缺失数据 合并及其他出现在常见数据库中的关系型运算。 我]]></content>
      <categories>
        <category>python工具</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Python进行数据分析之IPython]]></title>
    <url>%2F2018%2F02%2F20%2F%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[ipythonipython外加一个文本编辑器是较好的Python开发环境。当然，一款IDE也是不错的选择，但是有些IDE本身就是集成了IPython。 IPython本身并没有提供任何的计算和数据分析的功能，其主要目地就是在交互式计算和软件开发这两个方面最大化的提高生产力。他鼓励的是“执行 探索”的工作模式，而不是许多变成语言那种“编辑 编译 运行”的传统模式。 python基础启动就像标准的Python解释器那样输入ipython即可进入。12345C:\Users\diannaoλ ipythonPython 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]Type 'copyright', 'credits' or 'license' for more informationIPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help. 注意的是Python对象都被格式化的可读性很好，和print的输出形式有着显著的区别。 Tab键自动填充在shell中输入表达式时，只要按下Tab键，当前命名空间只要有与输入的字符串相匹配的变量就会被找出：而且还可以用在任何对象后面以及模块后面： 据说还可以填充路径字符串，本人无法成功，原因不明。 内省可以显示命名空间的信对象信息，以及函数的信息（一个问号会显示docstring，连个问号会显示整个函数的源代码）。 还可以作为正则表达式中去匹配： %run命令用来执行脚本文件。1%run 文件名.py 执行之后就可以在ipython shell中运用脚本里的全局变量、函数、模块等等。 执行剪切板中的代码利用%paste和%cpaste。%paste是直接复制后立即执行，%cpaste可以对此复制，方法是键入%cpaste后再点击右键会有提示，不停的在剪切板中添加新的元素就可以不断的复制粘贴，最后输入–，如果发现不对就输入Ctrl-c。 键盘快捷键 Ctrl-A 将光标移动到行首 Ctrl-E 将光标移动到行尾 Ctrl-K 删除从光标开始至行尾的文本 Ctrl-U 清除当前行的所有文本 Ctrl-L 清屏 魔术命令 %quickref 显示IPython的快速参考 %magic 显示所有魔术命令的详细文档 %debug 从最新的异常跟踪的底端进入交互式调试 %hist 打印命令的输入历史 %pdb 在异常发生后自动进入调试器 %time 报告某段程序执行时间 %timeit 多次执行某段程序，计算系综平均执行时间，对执行时间非常小的代码很有用。 %xdel variable 删除variable，并清除一切引用。 基于Qt的富GUI控制台安装了PyQt或Pyside，输入ipython qtconsole --pylab=inline这里我的版本是比较新的运行会出现警告，输入juyter qtconsole就行。 matplotlib集成与pylab模式输入ipython --pylab 使用命令历史ipython维护这一个小型的数据库，其中包含你执行过程中每条命令文本。 搜索并重用命令历史按住上或下（或者Ctrl-P或Ctrl-N）你就可以搜索你之前输入过的命令。 输入之前命令的部分字符再按住Crtl-R就能跳出与其匹配的命令。 输入变量和输出变量忘记把函数赋值给变量很让人郁闷，ipython会将一些输入和输出的引用保存在一些特殊的变量里，最近的两个输出结果分别保存在_和__中。 输入文本被保存在名为_ix的变量中，其中x代表行号，对应的输入变量被保存_x中，x的意义一样。exec _ix可以执行命令。%xdel和%reset可以解决释放内存不成功的问题。 与操作系统的交互 %bookmark 使用ipython的目录书签系统 %cd directory 将系统工作目录更改为书签系统 %pwd 返回系统的当前工作目录 %push directory 将当前目录压入堆栈，并转向目标目录 %poped 弹出栈顶目录，并转向目该目录 %dirs 返回一个含有当前目录栈的列表 shell命令和别名以感叹号开头的命令行表示其后的所有内容需要在shell中执行。比如：!ipython可以从ipython中退出然后进入到python标准交互系统中。还可以用%alias 别名 命令来为别名自定义别名，不过这是暂时的。 目录书签系统输入%bookmark db 目录就可以将目录用db来取代，下次直接输入cd db就能进入刚才你输入的目录。 IPython HTML Notebook在命令行中输入jupyter notebook注意这里我的版本，不是以前用的ipython notebook，而且后面也不能随便添加标注–pylab=inline，直接在juyter notebook中输入%pylab inline即可。]]></content>
      <categories>
        <category>python工具</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[module安装问题]]></title>
    <url>%2F2018%2F01%2F29%2Fmodule%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言：由于我为了方便，开始就安装了Anaconda，所以很多模块都已经装好了，遇到没有的模块，我们可以用pip install +模块名字，但是有的时候也会出现问题。 wordcloud安装出错你会发现在cmd的输入命令 pip install wordcloud 时会报错。解决方法： 在以下这个网址http://link.zhihu.com/?target=http%3A//www.lfd.uci.edu/%7Egohlke/pythonlibs/%23lxml下载你需要的的对应版本模块。 接着，把下载下来的文件原封不动的拷贝到你pyhon或者Anaconda的Scripts中，我的就是F:\Anaconda\Scripts 再到cmd中执行pip install F:\Anaconda\Scripts\刚才的文件名 这样就能下载成功！]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>模块安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫（3）]]></title>
    <url>%2F2018%2F01%2F26%2Fpython%E7%88%AC%E8%99%AB%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[之前的两节，我们把我们要的东西爬取下来，但是并没有做存储，下面我们将会讲解如何做存储。 保存到Excel123import pandasdf = pandas.DataFrame(final)df.to_excel('news.xlsx') 这是就可以在我们的目录当中找到news.xlsx的文件。 保存到数据库123import sqlite3with sqlite3.connect('news.sqlite') as db: df.to_sql('news',con = db) 可以利用pandas去sqlite中拿数据：12with sqlite3.connect('news.sqlite') as db: df2 = pandas.read_sql('SELECT * FROM news',con = db) 连上钱两次，就算是一个完整的爬虫过程了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫（2）]]></title>
    <url>%2F2018%2F01%2F26%2F%E7%88%AC%E8%99%AB%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[事前的章节只是负责把新闻的网页爬取下来，对于内文的需求还是不够的，所以这篇文章主要讲的是对于内文信息的爬取，包括标题、时间、作者、来源等等。 还和之前的爬去方法一样，利用Chrome F12就可以找到网页和获取响应的方法，之前有讲过。代码如下：12345import requestsfrom bs4 import BeautifulSoupres = requests.get("http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml")res.encoding = 'utf-8'soup = BeautifulSoup(res.text,'html.parser') 这样就获取了我们所需要的网页部分，继续进行标签的搜寻。 标题。在class=“main-title”的标签之中。 时间。在class=“date-source”中，但是都在span标签下，可以用contents将不同分层的标签下的东西取出。最后用datetime模块将时间字符串变成时间格式。 来源。和上面类似的操作。 内容。找到后会有分段的内容，利用for循环嵌套后就可以取出来，再用join连接成一个完整的部分。 评论数。你在doc中找不到，它在js中，而且还需要用json模块将数据变成我们容易处理的字典。具体代码如下：12345678910111213title = soup.select('.main-title')[0].textdate = soup.select('.date-source')[0].contents[1].textdt = datetime.strptime(date,'%Y年%m月%d日 %H:%M')source = soup.select('.date-source a ')[0].textbody = soup.select('.article p')[:-1]article = '\n'.join([d.text.strip() for d in body])mport jsoncomment = requests.get('http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\channel=gn&amp;newsid=comos-fyqyesy2133291&amp;\group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495')jd = json.loads(comment.text.lstrip('jsonp_1516947697495(').rstrip(')'))jd['result']['count']['total'] 评论数的获取页面链接中包含新闻id，在新闻本身的页面链接能找到。将它独立取出的方法有两个： 用split将链接变成多个块组成的list，然后再用索引取，最后用strip削掉我们不要的部分。 12newurl = 'http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml'newsid = newurl.split('/')[-1].lstrip('doc-i').rstrip('.shtml') 利用正则表达去匹配。 123import re m = re.search('doc-i(.*).shtml',newurl)m.group(1) 当然这是一个页面的评论数，写一个函数就能解决。利用字符串的format把上面获得的字符串填入大家共同的部分中。12345678910111213import re import jsonimport requestscommenturl = 'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\channel=gn&amp;newsid=comos-&#123;&#125;&amp;\group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495'def Getcount(newurl): m = re.search('doc-i(.*).shtml',newurl) newsid = m.group(1) comment = requests.get(commenturl.format(newsid)) jd = json.loads(comment.text.lstrip('jsonp_1516947697495(').rstrip(')')) return jd['result']['count']['total'] 将上面汇总成一个函数中，通过字典返回我们要的所有内文细节：12345678910111213141516171819import requestsfrom bs4 import BeautifulSoupimport re import jsonimport requestsdef News(newurl): article = &#123;&#125; res = requests.get(newurl) res.encoding = 'utf-8' soup = BeautifulSoup(res.text,'html.parser') article['title'] = soup.select('.main-title')[0].text date = soup.select('.date-source')[0].contents[1].text article['dt'] = datetime.strptime(date,'%Y年%m月%d日 %H:%M') article['source'] = soup.select('.date-source a ')[0].text body = soup.select('.article p')[:-1] article['articleBody'] = '\n'.join([d.text.strip() for d in body]) article['author'] = soup.select('.show_author')[0].text.lstrip('责任编辑：') article['commentcount'] = Getcount(newurl) return article 这里定义了一个网页链可以得到的所有内容信息，接下来就是考虑一个页面上的所有网页链接都爬下来。主要是通过在js中找到不同步的分页信息才得以了解所有的网页链接是什么。代码如下：12345678def parserlist(url): newdetails = [] res = requests.get(url) newurlnot = res.text.lstrip(' newsloadercallback(').rstrip(');') jd = json.loads(newurlnot) for d in jd['result']['data']: newdetails.append(News(d['url'])) return newdetails parserlist函数唯一的输入参数就是那个js中的网页连接（管理着一个页面的的所有新闻链接） 但是问题又来了，我们的分页可不止一个，所以我们继续观察分页的网页链接，发现page={数字}表示页数，那么我们可以写一个函数继续扩大工作量。代码如下：12345url = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&amp;cat_1=gnxw&amp;cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&amp;level==1||=2&amp;show_ext=1&amp;show_all=1&amp;show_num=22&amp;tag=1&amp;format=json&amp;page=&#123;&#125;&amp;callback=newsloadercallback&amp;_=1516960730249'final = []for i in range(1,10): newser = parserlist(url.format(i)) final.extend(newser) 最终的final就是我们要的9个页面的所有新闻内文。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫（1）]]></title>
    <url>%2F2018%2F01%2F25%2Fpython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[今天来简单的学习一下爬取网页的文档，然后归档。 第一个简单的爬虫利用chrome F12的检查功能对网页的回应进行读取，这里以新浪网为例。 获取到这些信息后，我们就能够写出一下代码：12345import requestsnew_url = 'http://news.sina.com.cn/society/res = requests.get(new_url)res.encoding = 'utf-8'print(res.text) 打印出来后表明没有问题，这是可以用到BeautifulSoup这个模块，讲刚才打印出来的东西丢进去，交给它处理，注意还需标明剖析器是html.parser。 12from bs4 import BeautifulSoupsoup = BeautifulSoup(res.text,'html.parser') 我们可以知道，在res里有很多的标签，我们需要的文字藏在固定的标签中，所以我们下面要做的就是取出我们需要的标签，需要用到soup的select方法。 12soup2 = soup.select('h2')print(soup2) 结果如下，是一个列表。 但是我们要爬取的内容的标签是以不同的id和class来区分的，这里涉及到css的相关内容，我们暂且不谈。只需要知道我们需要通过网页的检查了解我们的新闻藏在什么杨种类的标签之下。 现在可以知道我们的内容在news-item的class之下，所以我们运用select(‘.news-item’)的方法去获取，同样的方法，我们又可以知道，新闻的标题在h标签下，新闻的时间在值time的class之下，链接在h2标签的href中，写出一下代码。12345678from bs4 import BeautifulSoupsoup = BeautifulSoup(res.text,'html.parser')for link in soup.select('.news-item'): if len(link.select('h2')) &gt; 0: h2 = link.select('h2')[0].text time = link.select('.time')[0].text href = link.select('a')[0]['href'] print(time,h2,href) 注：text取的是标签的文字内容，而[‘href’]取的是标签内部的值。 最终结果如下：]]></content>
  </entry>
  <entry>
    <title><![CDATA[寒假]]></title>
    <url>%2F2018%2F01%2F24%2F%E5%AF%92%E5%81%87%2F</url>
    <content type="text"><![CDATA[我不知道为什么要写这个东西，只是闲着无聊，有不想动笔写日记，反正也没有喜欢的本子，就把闲置的博客拿来用用吧，希望未来自己看到这个能够有所感触。 我在从南京回来的路上听到一则鼓励写作的的录音，说写作并不用很好好的文笔只需要保持一直写就可以了。对于这个观点，我相信，原因很简单，我没有文笔，又喜欢写点东西，决定坚持。 我现在懂了，不是只有难得的事情，做出来了才伟大，简单的事情坚持一直做也是件很了不起的。突然想起来我的高中化学课的老师曾经和我们讲过某企业老板要求自己的员工在每天某个时间点准时起来做一分钟的高抬腿，而且他们真正的做到了。一件事，做一天你可能回说没什么，做一周也还好，一个月很厉害，一年也还好，三年四年。。。你会怎么觉得，我看到一则介绍爱情的短视频，一个男生为自己喜欢的女生每天削一个苹果，坚持了2300天，他们没有吵过架。 从此刻开始，我要为了自己想过的生活儿努力，坚持写作，每天。就和每天吃饭睡觉一样。寒假放假之前，我加了一个学习理财的群，我发现，他们一只都在教我们理财的重要性，大家还在学的那么起劲，我再管网几天，看看又没有干货，有没有的话就退出吧。今天看完了爬虫的一段教程，我觉讲的很有条理，明天继续。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Datastruct]]></title>
    <url>%2F2017%2F12%2F12%2Fdatastruct%2F</url>
    <content type="text"><![CDATA[线性表顺序表示DestroyList是将线性表的所有内容释放。线性表有三个属性，分别是线性表的空间基址、当前内容的长度length以及分配的长度size。（这里分配的长度需要大于内容的长度）DestroyList是讲所有的属性抹去初始值。 ClearList是将当前的内容长度置0。 isEmpty则是通过内容长度来判断线性表是否为空。 GetElem是将第i个元素取出来并返回。 FindElem是将表中元素与查找元素相同的下标输出。 对应的还有插入、删除、访问、历遍等等。。。。。 链式表示链式表示和顺序表示最大的不同就是，链式表示中逻辑位置相邻的数据他们需要物理位置也相邻。插入和删除不需要再将大量的数据做移动，减少算法的复杂度。 定义单链表时会有两个属性，1.数据，2.下一个节点的指针。 作业，编写代码实现链表反转和实现一元多项式的相加。 12/12/2017 11:46:54 AM]]></content>
  </entry>
  <entry>
    <title><![CDATA[考研前的课程复习安排]]></title>
    <url>%2F2017%2F11%2F27%2F%E8%80%83%E7%A0%94%E5%89%8D%E7%9A%84%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0%E5%AE%89%E6%8E%92%2F</url>
    <content type="text"><![CDATA[这学期可能是我大学期间的转折点，也将可能会是我大学阶段面对考试最困难的一个学期，因为我大三了，而且面临考验抉择，所以这个学期志在必得（一门都不许挂），下面是我最低限度的复习计划，针对每天整理出最少的复习量。 这学期的课程分为6门：通原，通线相对可以接受，交换，信息论，单片机，电磁场很难。 现在是第13周，距离考试还有4周不到的时间，我暂且粗略的估计是20天，还留七八天用来巩固。 通原一共13章，一天一章，从明天开始。 通线一共10章，一天一章，从明天开始。 电磁场一共8章，一天一章，从明天开始。 交换机一共11章，一天一章，从12月6日开始。 单片机一共11章，一天一章，从12月7日开始。 #信息论 #一共7章，一天一章，从12月11日开始。 这些课程除外还有一节毛概，一节通创，一节就业指导，以及一节实验。 综合起来将会有10门功课，加油吧，就这么多了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F11%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
