<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python爬虫（2）]]></title>
    <url>%2F2018%2F01%2F26%2F%E7%88%AC%E8%99%AB%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[事前的章节只是负责把新闻的网页爬取下来，对于内文的需求还是不够的，所以这篇文章主要讲的是对于内文信息的爬取，包括标题、时间、作者、来源等等。 还和之前的爬去方法一样，利用Chrome F12就可以找到网页和获取响应的方法，之前有讲过。代码如下：12345import requestsfrom bs4 import BeautifulSoupres = requests.get("http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml")res.encoding = 'utf-8'soup = BeautifulSoup(res.text,'html.parser') 这样就获取了我们所需要的网页部分，继续进行标签的搜寻。 标题。在class=“main-title”的标签之中。 时间。在class=“date-source”中，但是都在span标签下，可以用contents将不同分层的标签下的东西取出。最后用datetime模块将时间字符串变成时间格式。 来源。和上面类似的操作。 内容。找到后会有分段的内容，利用for循环嵌套后就可以取出来，再用join连接成一个完整的部分。 评论数。你在doc中找不到，它在js中，而且还需要用json模块将数据变成我们容易处理的字典。具体代码如下：12345678910111213title = soup.select('.main-title')[0].textdate = soup.select('.date-source')[0].contents[1].textdt = datetime.strptime(date,'%Y年%m月%d日 %H:%M')source = soup.select('.date-source a ')[0].textbody = soup.select('.article p')[:-1]article = '\n'.join([d.text.strip() for d in body])mport jsoncomment = requests.get('http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\channel=gn&amp;newsid=comos-fyqyesy2133291&amp;\group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495')jd = json.loads(comment.text.lstrip('jsonp_1516947697495(').rstrip(')'))jd['result']['count']['total'] 评论数的获取页面链接中包含新闻id，在新闻本身的页面链接能找到。将它独立取出的方法有两个： 用split将链接变成多个块组成的list，然后再用索引取，最后用strip削掉我们不要的部分。 12newurl = 'http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml'newsid = newurl.split('/')[-1].lstrip('doc-i').rstrip('.shtml') 利用正则表达去匹配。 123import re m = re.search('doc-i(.*).shtml',newurl)m.group(1) 当然这是一个页面的评论数，写一个函数就能解决。利用字符串的format把上面获得的字符串填入大家共同的部分中。12345678910111213import re import jsonimport requestscommenturl = 'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\channel=gn&amp;newsid=comos-&#123;&#125;&amp;\group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495'def Getcount(newurl): m = re.search('doc-i(.*).shtml',newurl) newsid = m.group(1) comment = requests.get(commenturl.format(newsid)) jd = json.loads(comment.text.lstrip('jsonp_1516947697495(').rstrip(')')) return jd['result']['count']['total'] 将上面汇总成一个函数中，通过字典返回我们要的所有内文细节：12345678910111213141516171819import requestsfrom bs4 import BeautifulSoupimport re import jsonimport requestsdef News(newurl): article = &#123;&#125; res = requests.get(newurl) res.encoding = 'utf-8' soup = BeautifulSoup(res.text,'html.parser') article['title'] = soup.select('.main-title')[0].text date = soup.select('.date-source')[0].contents[1].text article['dt'] = datetime.strptime(date,'%Y年%m月%d日 %H:%M') article['source'] = soup.select('.date-source a ')[0].text body = soup.select('.article p')[:-1] article['articleBody'] = '\n'.join([d.text.strip() for d in body]) article['author'] = soup.select('.show_author')[0].text.lstrip('责任编辑：') article['commentcount'] = Getcount(newurl) return article 这里定义了一个网页链可以得到的所有内容信息，接下来就是考虑一个页面上的所有网页链接都爬下来。主要是通过在js中找到不同步的分页信息才得以了解所有的网页链接是什么。代码如下：12345678def parserlist(url): newdetails = [] res = requests.get(url) newurlnot = res.text.lstrip(' newsloadercallback(').rstrip(');') jd = json.loads(newurlnot) for d in jd['result']['data']: newdetails.append(News(d['url'])) return newdetails parserlist函数唯一的输入参数就是那个js中的网页连接（管理着一个页面的的所有新闻链接） 但是问题又来了，我们的分页可不止一个，所以我们继续观察分页的网页链接，发现page={数字}表示页数，那么我们可以写一个函数继续扩大工作量。代码如下：12345url = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&amp;cat_1=gnxw&amp;cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&amp;level==1||=2&amp;show_ext=1&amp;show_all=1&amp;show_num=22&amp;tag=1&amp;format=json&amp;page=&#123;&#125;&amp;callback=newsloadercallback&amp;_=1516960730249'final = []for i in range(1,10): newser = parserlist(url.format(i)) final.extend(newser) 最终的final就是我们要的9个页面的所有新闻内文。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫（1）]]></title>
    <url>%2F2018%2F01%2F25%2Fpython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[今天来简单的学习一下爬取网页的文档，然后归档。 第一个简单的爬虫利用chrome F12的检查功能对网页的回应进行读取，这里以新浪网为例。 获取到这些信息后，我们就能够写出一下代码：12345import requestsnew_url = 'http://news.sina.com.cn/society/res = requests.get(new_url)res.encoding = 'utf-8'print(res.text) 打印出来后表明没有问题，这是可以用到BeautifulSoup这个模块，讲刚才打印出来的东西丢进去，交给它处理，注意还需标明剖析器是html.parser。 12from bs4 import BeautifulSoupsoup = BeautifulSoup(res.text,'html.parser') 我们可以知道，在res里有很多的标签，我们需要的文字藏在固定的标签中，所以我们下面要做的就是取出我们需要的标签，需要用到soup的select方法。 12soup2 = soup.select('h2')print(soup2) 结果如下，是一个列表。 但是我们要爬取的内容的标签是以不同的id和class来区分的，这里涉及到css的相关内容，我们暂且不谈。只需要知道我们需要通过网页的检查了解我们的新闻藏在什么杨种类的标签之下。 现在可以知道我们的内容在news-item的class之下，所以我们运用select(‘.news-item’)的方法去获取，同样的方法，我们又可以知道，新闻的标题在h标签下，新闻的时间在值time的class之下，链接在h2标签的href中，写出一下代码。12345678from bs4 import BeautifulSoupsoup = BeautifulSoup(res.text,'html.parser')for link in soup.select('.news-item'): if len(link.select('h2')) &gt; 0: h2 = link.select('h2')[0].text time = link.select('.time')[0].text href = link.select('a')[0]['href'] print(time,h2,href) 注：text取的是标签的文字内容，而[‘href’]取的是标签内部的值。 最终结果如下：]]></content>
  </entry>
  <entry>
    <title><![CDATA[寒假]]></title>
    <url>%2F2018%2F01%2F24%2F%E5%AF%92%E5%81%87%2F</url>
    <content type="text"><![CDATA[我不知道为什么要写这个东西，只是闲着无聊，有不想动笔写日记，反正也没有喜欢的本子，就把闲置的博客拿来用用吧，希望未来自己看到这个能够有所感触。 我在从南京回来的路上听到一则鼓励写作的的录音，说写作并不用很好好的文笔只需要保持一直写就可以了。对于这个观点，我相信，原因很简单，我没有文笔，又喜欢写点东西，决定坚持。 我现在懂了，不是只有难得的事情，做出来了才伟大，简单的事情坚持一直做也是件很了不起的。突然想起来我的高中化学课的老师曾经和我们讲过某企业老板要求自己的员工在每天某个时间点准时起来做一分钟的高抬腿，而且他们真正的做到了。一件事，做一天你可能回说没什么，做一周也还好，一个月很厉害，一年也还好，三年四年。。。你会怎么觉得，我看到一则介绍爱情的短视频，一个男生为自己喜欢的女生每天削一个苹果，坚持了2300天，他们没有吵过架。 从此刻开始，我要为了自己想过的生活儿努力，坚持写作，每天。就和每天吃饭睡觉一样。寒假放假之前，我加了一个学习理财的群，我发现，他们一只都在教我们理财的重要性，大家还在学的那么起劲，我再管网几天，看看又没有干货，有没有的话就退出吧。今天看完了爬虫的一段教程，我觉讲的很有条理，明天继续。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Datastruct]]></title>
    <url>%2F2017%2F12%2F12%2Fdatastruct%2F</url>
    <content type="text"><![CDATA[线性表顺序表示DestroyList是将线性表的所有内容释放。线性表有三个属性，分别是线性表的空间基址、当前内容的长度length以及分配的长度size。（这里分配的长度需要大于内容的长度）DestroyList是讲所有的属性抹去初始值。 ClearList是将当前的内容长度置0。 isEmpty则是通过内容长度来判断线性表是否为空。 GetElem是将第i个元素取出来并返回。 FindElem是将表中元素与查找元素相同的下标输出。 对应的还有插入、删除、访问、历遍等等。。。。。 链式表示链式表示和顺序表示最大的不同就是，链式表示中逻辑位置相邻的数据他们需要物理位置也相邻。插入和删除不需要再将大量的数据做移动，减少算法的复杂度。 定义单链表时会有两个属性，1.数据，2.下一个节点的指针。 作业，编写代码实现链表反转和实现一元多项式的相加。 12/12/2017 11:46:54 AM]]></content>
  </entry>
  <entry>
    <title><![CDATA[考研前的课程复习安排]]></title>
    <url>%2F2017%2F11%2F27%2F%E8%80%83%E7%A0%94%E5%89%8D%E7%9A%84%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0%E5%AE%89%E6%8E%92%2F</url>
    <content type="text"><![CDATA[这学期可能是我大学期间的转折点，也将可能会是我大学阶段面对考试最困难的一个学期，因为我大三了，而且面临考验抉择，所以这个学期志在必得（一门都不许挂），下面是我最低限度的复习计划，针对每天整理出最少的复习量。 这学期的课程分为6门：通原，通线相对可以接受，交换，信息论，单片机，电磁场很难。 现在是第13周，距离考试还有4周不到的时间，我暂且粗略的估计是20天，还留七八天用来巩固。 通原一共13章，一天一章，从明天开始。 通线一共10章，一天一章，从明天开始。 电磁场一共8章，一天一章，从明天开始。 交换机一共11章，一天一章，从12月6日开始。 单片机一共11章，一天一章，从12月7日开始。 #信息论 #一共7章，一天一章，从12月11日开始。 这些课程除外还有一节毛概，一节通创，一节就业指导，以及一节实验。 综合起来将会有10门功课，加油吧，就这么多了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F11%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
