<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何自我保护]]></title>
    <url>%2F2018%2F05%2F01%2F%E5%A6%82%E4%BD%95%E8%87%AA%E6%88%91%E4%BF%9D%E6%8A%A4%2F</url>
    <content type="text"><![CDATA[自从我看了武汉理工的那个研三的学长跳楼之后，我的心理五味杂陈，甚至有些矛盾。我认为在这样一个年龄选择轻生实在是不负责的一种表现，自己一走了之，留下的父母无尽的痛苦。但是看到他被导师折磨到已经介乎病态的样子，我有不禁为他感到惋惜。 我很疑惑，为什么我们看到他和导师之间的对话记录后，都觉得其实不用在意那些有的没的，导师毕竟只是学校里的一颗螺丝钉，凭什么能够操控一个学生，但导师提出威胁的时候，果断的提出拒绝，大不了退学，那有如何？ 直到我在网上看到一篇文章讲了他这个问题，我觉得有些收获。在所有的人际关系中，都存在着一种双方对这种关系“边界”的认知定势。 像陶崇园这样处境的其实大有人在，比如在父母的关系中，如果父母长期干涉我们的决定和判断，喜欢我们按着他们要求去生活，时间久了，那么父母就会有这样的思维定势：我的孩子非常听话，我让他做什么他就会做什么。我们自己也会有一种认知定势：爸妈说的话不可以违抗，我必须得遵循。 这种定势在旁人看来会很荒谬，但是在当事人眼中这些确实理所应当，天经地义。这就是旁观者和当事人对于所处境地的认知的不同，最根本的不是当事人在客观上有多少能力去反抗和逃避这种压力，而是那些压力早已经将他压垮，甚至可以说是出于濒临崩溃的状态。 如果这样去理解的话，那似乎也不难想到为什么学长宁可选择跳楼也不去选择反抗，因为他自我的心理防护完全崩塌了。我个人也有个感悟：这个世界上，只有一种否定会将人击垮，就是自我否定，自己心理崩塌。可能有人会说你很丑，如果你自己没法接受自己丑，那才会有伤害；可能你失败了很多次，被人都在说你不能成功，而如果你自己很享受自己在失败中自我反省的过程，或许还有积极的效果。那我自己来说，我最怕的一个点就是别人说我瘦，因为我真的瘦，其实这句评判如果足够客观可以不带任何的感情色彩，但是由于我自己不太能接受自己为什么吃的也多，还是这么瘦，这样别人每说一次，都等于加剧我的否定。这在旁人看来很不起眼，但是我想说我不是矫情，我不喜欢听这句话，想说也别让我听见。当然，我现在释怀了，这或许就是我的缺陷，反正不会影响我人格的发展，我何必耿耿于怀。 认知定势其实很难改变的，为什么呢？原因如下: 自我保护。在关系中弱势的那一方，因为长期的被压迫，通常产生一种不敢去反抗的“习得性”无助。当我们试图在逻辑上去说服，这个时候他们会想出很多理由去反驳。当一个人开始无条件的秉持一个绝对的信念时，其实你没法从逻辑上去说服他。他们往往不想承受一种痛苦，而选择另一种可以承受的痛苦，以至于无法自拔。 环境影响。周围的人和事或多说少会对我们自己的行为有影响。任何一个团体都会有一定的筛选的作用。 合理化。当我们认定一件事或者想法，会找无数理由来支持我们的判断。 来自施虐者的威胁。这就是弱势者无法改变的现实原因。 下面讨论一个人际弱势者如何重新找回在人际关系中地位和尊严。 本质需要去尝试理解：破执。人只要陷入“执着”，就不可避免的被别人操控。任何一个弱势者，必然都是有着或隐或显的强烈执着。 执着的具体对象并不重要，重要的是执着本身。不官方的解释是：执着源自早年生活的不安全感。因为有一种无处不在的不安全感，所以需要无时无刻都投入大量的精力去关注周围的事物，确保事物不会发展到自己预期之外的圈子。所以这种执着本质就是一种想通过自己的努力，控制周围环境，让自己更安全。 于是有了一个逻辑：不安全感-&gt;想努力去控制外界-&gt;很容易对外界事物产生执着-&gt;有了执着就很容易被别人控制-&gt;一旦被威胁就会很焦虑-&gt;基于焦虑会做出伤害自己的事,产生更多压力-&gt;压力累计后压垮 那么追根溯源，就是不安全感。其实我们要做的就是学会接受自己的不安全感，适应它，承认它，但是不受它的影响去选择自己的生活。 那些心理健康的人，他们就是对引起焦虑的想法不去过多的关注，让念头来去自如，他们将关注点放在了真实的生活上。 那些人际弱势的人，因为并没有真正去面对自己的不安全感，把不安全的境况想象的无比可怕。这样会无止境的将这种不安全感排除在外。 从全神贯注的执着中跳出，看到人生更多的可能性。 想跳出来也不难，只要一句话：去tma的，老子不干了！谁不爱自己，痛苦是自己找的。]]></content>
      <categories>
        <category>心理辅导</category>
      </categories>
      <tags>
        <tag>自我保护</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析：产品经理的基础]]></title>
    <url>%2F2018%2F04%2F04%2F%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E7%9A%84%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[在腾讯鹅产我谈会的鼓励下，我想好好准备我的面试（虽然我还没过笔试），选择的方式就是在人人都是产品经理这个网站上，我说实话自己离自己的梦想的距离还是非常远的，索性在为自己到处都是不足而焦虑，不如务实一点，做点自己觉得有意思的事。 埋点这个概念似乎在之前有听过，是学网站运营的时候，看到的。解释是这样的：根据业务需求、产品需求去在网页上设计埋点，记录数据，再通过软件工具把数据提取出来，用于专业人员的数据分析，驱动产品的优化，迭代。 大致的过程是： When1、了解业务的各方面要求。 在什么场景下？ 面对哪些客户？ 解决什么问题？ 解决的程度？（能给到多少价值） 2、具体讨论问题的轻重缓急。3、将抽象的问题进行量化。 给定量化指标 评价体系指标 这些都解决完了就能够进行埋点了。 How为了便于后期的维护，降低推翻重来的可能性，就要按照规范埋点。简单的埋点规则就是将一个事件分成分成三个表：点击事件集合，曝光事件集合，用户停留时间事件集合。再对这三张表分别三个表单：首页事件，列表页事件，详情事件。如果版本更新就该把之前的埋点进行copy，再用颜色标明两个之间大的变化。 关于埋点的细节： 功能名称解释在什么页面下的什么功能。 中文字段名称解释在什么功能模块的什么位置上。 ID数据的主键，往往是按照一定的规则的。 事件类型点击还是曝光，还是其他 Key——value对于不同类型的不同位置，需要设置，可以一对一和一对多。 规则表述对具体的时间规则进行描述。 备注每一次改动都应该有所记录。 以上可以算是对埋点有了一个初步的认知。接下来我再来深入Key—value的讲解。 Key——value详解设这样一一个场景：某汽车互联网公司，领导需要产品经理对销售线索指标进行监控，如果有超过5%的波动，就进行数据值的记录和汇报。 根据领导的需求，假设销售线索指标里电话咨询和电话砍价这两个行为字段比较关键，对于它们的检测可以通过详情页面的两个按钮。转到详情页面的来源假设有两个，页面A和页面B。 一般设置Key——value有两个方式。 每一个事件都设置一个ID。 同一属性的不同事件可以设置同一个Key，然后不同时间用value去区分。 如果是每一个都去设置一个ID，那么我想说的是一下缺点: 来源和按钮（行为）如果增加的话，存的的量是很大的。 数据分析的时候去取数据，写SQL查询语句的时候，ID值需要写很多。 这样会徒增很多的工作量。 利用Key——value来区分事件：把事件分成来源和类型两个键值，就能很大程度上去把数据进行分类，方便日后的查询、使用和维护。 得到两个结论： 同种属性的多个事件，建议设一个ID，并通通过Key——value进行区分。 不同属性的多个事件，设置不同的ID,不建议使用Key——value。 统计基础我们首先要树立一个意识，就是我们在实际应用中的研究的对象总体往往数据量很大，我们不可能研究总体，统计学中就有了一个方法，用样本来估计总体。 这里可以用辛钦大数定律来解释，当有一个样本集是一个随机变量序列，每个随机变量都是独立同分布的，自然能想到能否通过这些样本去研究总体的一些特征？ 答案当然是肯定的，当样本的数据量增多时，样本的均值接近总体概率的会越来越来大。 以上就是为什么可以用样本数据去研究总体数据的原因了。 均值均值可以衡量一个数据集在某一个数值型字段上的水平.但是有的时候数据中有异常值的时候,平均值的评价效果大大降低,即大部分的数据都低于平均值或高于平均值,这就出现了右偏和左偏的数据: 右偏就是因为异常值太大,将均值往右边拉. 左偏就是因为异常值太小,将均值往左边拉. 我们预期的数据都是正态的,所以这个时候要么选用其他的观测量,要么就是去异常值. 中位数中位数就是一个可以克服异常值的观测变量.同时还有四分位数. 2018/4/4 14:20:31]]></content>
      <categories>
        <category>产品</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面经demo]]></title>
    <url>%2F2018%2F04%2F02%2F%E9%9D%A2%E7%BB%8Fdemo%2F</url>
    <content type="text"><![CDATA[在ppv网上发现一篇帖子，专门是对付数据分析面试的，我呢觉得自己也该准备一些面试的东西了，哪怕自己练面试的机会也没有。 1、你处理过的最大的数据量？你是如何处理他们的？处理的结果我处理过最大的数据量有88万条记录，用CSV文件储存的，大小是400M左右。内容是关于借贷的，来源与Kaggle。 导入：我开始尝试用Excel打开，发现那是‘一项浩大的工程’，所有我用Python的第三方库Pandas的read_csv命令打开的，也花了不到一分钟的时间。而且对数据的格式也进行了限制。 数据探索：通过Python的describe()和数据的指标字典文件，对有用的变量我进行了筛选，删除了用户ID等对评定借贷人信用不会有影响的字段。 数据缺省值处理：我删除了缺省记录超过80%的字段。对于缺省较少的，如果是字串我就用采用前一个记录的值补上，如果是数值我就用众数。 数据重复值处理：文件里有些字段记录的重复率很高，那么我选择删除。最后得到可以运用于logistic模型 2、告诉我二个分析或者计算机科学相关项目？你是如何对其结果进行衡量的？我有一个用户评分卡模型，这是一个分类的项目，评定用户信用度的，因为是一个二分类，所有我选择的是ROC曲线和AUC值来评定的。将测试集中的目标变量的值和预测值进行对比，计算出不同阈值下的真阳率和假阳率绘制ROC，并且根据ROC计算。 3、什么是：提升值、关键绩效指标、强壮性、模型按合度、实验设计、2/8原则？提升值是：在原有的数值上数值上增加的数值。关键绩效指标：从对象的关键成果中提取的指标，用以衡量对象的绩效。强壮性：软件或者系统对于规定输入范围以外情况的处理能力。模型的拟合度：是预测的数据与实际的吻合度。实验设计：一种安排实验和实验数据的数理统计方法。2/8原则：重要的都是少数，而占多数的往往都是次要的。 4、什么是：协同过滤、n-grams, map reduce、余弦距离？协同过滤：是一种推荐算法，利用]]></content>
      <categories>
        <category>春招</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>面经</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析师的面试准备]]></title>
    <url>%2F2018%2F03%2F31%2F%E9%9D%A2%E8%AF%95%2F</url>
    <content type="text"><![CDATA[我觉得自己既然选择放弃考研，转而找工作就应该有找工作应该有的态度。至少我不想自己未来后悔，无论未来找不找的到，起码这篇长文应该能说明一切。我可以承受失败，但我不能接受自己坐以待毙，不努力后的失败。下面开始我的正文。 我找的是数据分析岗，截至目前2018年3月31日，我看到的国内的大公司，包括BAT,网易，头条，七牛云，京东，小米，岗位主要分布在技术岗，产品运营岗，产品设计岗这三个岗位。 浅尝笔试： 第一家的腾讯模拟笔试，笔试很基础，但是你乍一眼看上去就是很难，比如，他会考你相关关系和因果关系的区别（这道题分很高），还有排列组合，二叉树的前中后序遍历，SQL语句，以及统计的基础内容。 第二家的网易实习笔试，题目开始有难度，不过这些难度都是虚的，20题选择，3题问答，20题的部分主要看个人能力，当然也有数据分析相关的统计和数据结构。3题问答就是数据分析的基本功了，要会sql的高级查询，一定要到网站上刷题！！！还有就是一些用户特征分析的操作，我觉得可以在日常就留给心眼，做在脑中呈现完整的数据分析过程，遇到这种问题就能得心应手。 经过这两门的笔试，我有了些自己的感悟： 不要只注重刷死题，数据分析本来就是一个注重分析的岗位，大学里应付期末考试的那套方法在这里不灵，我们要做的就是多看，看图表；多思，思业务和技术的联系；多总结，业务难点。 基础知识必须夯实，统计，sql或者excel，Python或者R，机器学习算法，和业务相关的经济知识。 看公司都有什么样的业务，对应会出现什么业务需求，不同的业务需求要有什么的对策，要用什么样的分析方法。 2018/3/31 10:40:38]]></content>
      <categories>
        <category>数据分析职业</category>
      </categories>
      <tags>
        <tag>面试或笔试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python3和Python2的区别]]></title>
    <url>%2F2018%2F03%2F08%2Fpython3%E5%92%8CPython2%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[小编在学习apriori算法的时候，发现了一些Python两种版本的差别，虽然不大，但是还是记下来比较好，万一以后要用也好找，毕竟python3在未来还是会取代python2的，但是学习的时候还是python2比较稳定。 字典dict的has_key()python3中已经不用这个方法了。1234567dict_1 = &#123;k:'g', e:'h', y:'j'&#125;#对于py2if dict_1.has_key(k):#对于py3if k in dict_1: 内建函数map()py3中无法返回list12345#py2中直接返回listlen(map对象)#py3需要进行一次转换len(list(map对象)) 2018/3/8 13:35:12]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python版本区别</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库与数据对象]]></title>
    <url>%2F2018%2F03%2F02%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[创建和维护数据库数据库中的一些概念之间的关系如以下的思维导图： 创建数据库创建数据库需要注意一下的几个步骤： 注明数据库的名称。 看清有几个数据文件，日志文件。 数据文件是否需要有文件组的安排。 初始化文件的各种属性。 1234567891011121314151617181920212223CREATE DATABASE 数据库名称ON PRIMARY (NAME = 逻辑文件名 FILENAME = 物理文件名 SIZE = 初始大小 MAXSIZE = 最大大小 FILEGROWTH = 增长 )FILEGROUP 用户自定义的文件组名 (NAME = 逻辑文件名 FILENAME = 物理文件名 SIZE = 初始大小 MAXSIZE = 最大大小 FILEGROWTH = 增长 )LOG ON (NAME = 逻辑文件名 FILENAME = 物理文件名 SIZE = 初始大小 MAXSIZE = 最大大小 FILEGROWTH = 增长 ) 修改数据库扩大数据库空间如果没有设置数据库自动增长的话，那么数据库在使用一段时间后会出现空间不够的情况，那么就可以通过修改数据库空间来解决。 扩大数据库空间有两个方法： 扩大已有文件的大小。 数据库添加新的文件。 123456789101112ALTER DATABASE 数据库的名称#ADD FILE (NAME = 逻辑文件名 FILENAME = 物理文件名 SIZE = 初始大小 MAXSIZE = 最大大小 FILEGROWTH = 增长 )#MODIFY FILE (NAME = 逻辑文件名 SIZE = 初始大小 ) 注意：增加文件还可以讲文件添加到指定的文件组组中。 收缩数据文件释放文件中未使用的空间，并将此空间还给系统空间，数据文件和日志文件都可以收缩。收缩的方法有： 手动收缩 设置数据库参数自动收缩 手动收缩 收缩整个数据库的大小。 1234DBBC SHRINKDATABASE (数据库名称或者ID， 大小比例, NOTRUNCATE或者TRUNCATEONLY) #要么把空间给系统，要么不保留 收缩指定文件的大小 12345DBBC SHRINKDATABASE (数据库名称或者ID， 大小比例, EMPTYFILE, #该文件为空，将原有文件转移到同一文件组的其他文件中。 NOTRUNCATE或者TRUNCATEONLY) 自动收缩就是简单的将数据库的AUTO_SHRINK设置为True，默认情况下是False。 添加和删除数据库文件可以通过在数据库中添加文件来扩大数据库的大小，也可以通过删除数据库的文件来减小数据库的大小。 添加文件组中各个文件的数据量大小就是和和文件的可用空间成正比，所以添加数据文件的时候数据库会先使用新加的文件。相比之下，日志文件就不会这样，由于文件不存在文件组，所以文件之间是彼此独立，总是先写第一个日志文件，当填充满了以后再写第二个，所以，当添加日志文件的时候不会立刻使用该文件，直到其他文件被填满。 删除只有当文件完全为空的时候，才能够删除。日志文件比较复杂，需要截断日志或者备份日志才可以。12ALTER DATABASE 数据库名称 REMOVE FILE 文件名 分离和附加数据库]]></content>
      <categories>
        <category>数据库技术</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>数据库操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是数据化运营]]></title>
    <url>%2F2018%2F03%2F01%2F%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[1.1 现代营销理论的发展4P到4C4P指product、Price、Place、Promotion。product：产品注重功能，强调卖点。price：价格需要根据不同的市场定位。place：渠道注重分销商的培养和销售网络的建设。promotion：企业通过改变销售行为来刺激消费，以短期的行为促进消费增长。 4P理论的核心是Product。 随着时代的发展，商品丰富起来，市场竞争也日益激烈。在21实际消费者成为了商业的核心。这时4C理论兴起。4C是指consumer、cost、communication、convenience。consumer：是指消费者的需求和欲望。cost：是指消费者得到满足的成本。communication：是指与用户的沟通。convenience：是指用户购买的方便性。 4C理论的核心是消费者。 4C到3P3C大数据时代的来临，4C理论再次落后于时代发展的需求，海量数据的堆积和存储等迫使现代企业不得不寻找更适合、更可控、更可量化的营销思路和方法论。3P3C的六要素：Probability：营销、运营活动以概率为核心，追求精细化和精准率。Product：产品prospect：目标用户Creative：创新Channel：渠道Cost：成本 数据分析挖掘所支撑的目标响应概率是核心。这里的目标响应概率不应狭义理解为仅仅是预测响应模型之类的响应概率，它有更加宽泛的含义。从宏观上理解，概率可以是特定消费者整体上的概率或可能性。比如，我们用卡方检验某个特定的类别的消费群体在某一个消费行为指标上具有显著性特征。从微观上理解，概率可以具体到某一个特定消费者的“预期响应概率”，比如用逻辑回归算法搭建一预测响应模型，得到每个用户的预计响应概率。 数据化运营的主要内容数据化运营业内没有统一的定义，但是其基本要素和核心是一致的，那就是:“以企业级海量数据的存储和挖掘应用为核心支持，企业全员参与的，以精准、细分和精细化为特点的企业运营制度和战略。” 浅层次的理解的话，就是在企业常规运营的基础上革命性地增添数据分析和挖掘的精准支持。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据化化运营</tag>
        <tag>营销</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据导入]]></title>
    <url>%2F2018%2F02%2F25%2FMySQLL%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[在安装完MySQL数据库后，数据库中的数据呈现最初始的状态，所以需要向其中填充数据，但是这是数据准备阶段的工作。下面简单介绍一下： 构建数据库和table 首先需要进入数据库。在管理员命令窗口输入net start mysql5.7.21。 然后再登陆mysql。 1mysql -u root -p 进入后再创建database。 1CREATE DATABASE test2 #默认是以utf8编码 进入test2这个database中。 1use test2 再进行CSV文件的导入。以table的形式。 123456CREATE TABLE TEST_BOOKS1( team_name varchar(100), Wins int, Draws int, Losses int); 创建table的格式如下：变量名 类型 限制以上的实例就是我的表格结构的概括，可以根据自己的需求自行修改参数： 导入数据导入将自己文件里的数据导入到这个构造好的table中。123456LOAD DATA INFILE 'C:\Users\diannao\Desktop\2015-16-premier-league.csv'INTO TABLE TEST_BOOKS1FIELDS TERMINATED BY ','ENCLOSED BY '"'LINES TERMINATED BY '\n'IGONRE 1 ROWS; 错误解决这里需要提一下，如果运行不成功，并且显示的错误是secure-file-priv有问题，那么就需要在my.ini文件中的[mysqld]下进行声明secure-file-priv=,这里的意思就是对于导入和导出不去进行干扰。注意，声明完成后，还需要在管理员命令窗口下对mysql5.7.21进行重启，才能有效。 检验检验的方法SELECT * FROM TEST_BOOKS1到这里就算导入成功了！]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>数据准备</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL安装与配置]]></title>
    <url>%2F2018%2F02%2F24%2FMySQL%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[我们首先可以在Mysql的官网上找到我们的需要下载的文件。选择符合自己电脑的版本。 这些都是不需要安装的压缩包，这里就讲一下怎么用这个压缩包安装。 解压文件，在文件里添加初始化文件my.ini。先新建txt文档然后再更名为my.ini。在此文档里添加一段初始化的代码。12345678910111213141516171819[mysql]#设置mysql客户端默认字符集default-character-set=utf8[mysqld]#设置3306端口port=3306#设置mysql的安装目录basedir=D:\Program Files\mysql-5.7.21-winx64#设置mysql数据库的数据库的存放目录datadir=D:\Program Files\mysql-5.7.21-winx64\data#允许最大连接数max_connections = 200#服务端使用的字符集默认为8比特的latin1字符集character-set-server=utf8#开启查询缓存explicit_defaults_for_timestamp = trueskip-grant-tables#创建新表将使用的默认存储引擎default-storage-engine=INNODB 需要注意的是：上面第二个【】里是mysqld。 配置系统变量。在系统变量里添加MYSQL_HOME,路径填你解压文件所在的目录下，我的是D:\Program Files\mysql-5.7.21-winx64。然后在系统path里添加：%MYSQL_HOME%\bin。 以管理员身份运行命令终端cmd。执行一下代码： 123d:cd D:\Program Files\mysql-5.7.21-winx64\binmysqld --initialize --user=mysql --console 安装服务。 1mysqld --install MySQL5.7.21 启动服务。 1net start MySQL5.7.21 修改密码。 1set password for root@local=password('你的密码') 每次登陆mysql，就输入1mysql -u root -p]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Python进行数据分析之Pandas]]></title>
    <url>%2F2018%2F02%2F21%2F%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPandas%2F</url>
    <content type="text"><![CDATA[2008年开始就有pandas库，构建者构造它的原因就是没有任何一个工具可以满足他的工作需求。 具备按轴自动或显示数据的对齐的功能的数据结构。 集成时间序列功能。 既能处理时间序列数据也能处理非时间序列数据的数据结构。 数据运算和约简可以根据不同的元素数据执行。 灵活处理缺失数据 合并及其他出现在常见数据库中的关系型运算。 我]]></content>
      <categories>
        <category>python工具</category>
      </categories>
      <tags>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Python进行数据分析之IPython]]></title>
    <url>%2F2018%2F02%2F20%2F%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[ipythonipython外加一个文本编辑器是较好的Python开发环境。当然，一款IDE也是不错的选择，但是有些IDE本身就是集成了IPython。 IPython本身并没有提供任何的计算和数据分析的功能，其主要目地就是在交互式计算和软件开发这两个方面最大化的提高生产力。他鼓励的是“执行 探索”的工作模式，而不是许多变成语言那种“编辑 编译 运行”的传统模式。 python基础启动就像标准的Python解释器那样输入ipython即可进入。12345C:\Users\diannaoλ ipythonPython 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]Type 'copyright', 'credits' or 'license' for more informationIPython 6.1.0 -- An enhanced Interactive Python. Type '?' for help. 注意的是Python对象都被格式化的可读性很好，和print的输出形式有着显著的区别。 Tab键自动填充在shell中输入表达式时，只要按下Tab键，当前命名空间只要有与输入的字符串相匹配的变量就会被找出：而且还可以用在任何对象后面以及模块后面： 据说还可以填充路径字符串，本人无法成功，原因不明。 内省可以显示命名空间的信对象信息，以及函数的信息（一个问号会显示docstring，连个问号会显示整个函数的源代码）。 还可以作为正则表达式中去匹配： %run命令用来执行脚本文件。1%run 文件名.py 执行之后就可以在ipython shell中运用脚本里的全局变量、函数、模块等等。 执行剪切板中的代码利用%paste和%cpaste。%paste是直接复制后立即执行，%cpaste可以对此复制，方法是键入%cpaste后再点击右键会有提示，不停的在剪切板中添加新的元素就可以不断的复制粘贴，最后输入–，如果发现不对就输入Ctrl-c。 键盘快捷键 Ctrl-A 将光标移动到行首 Ctrl-E 将光标移动到行尾 Ctrl-K 删除从光标开始至行尾的文本 Ctrl-U 清除当前行的所有文本 Ctrl-L 清屏 魔术命令 %quickref 显示IPython的快速参考 %magic 显示所有魔术命令的详细文档 %debug 从最新的异常跟踪的底端进入交互式调试 %hist 打印命令的输入历史 %pdb 在异常发生后自动进入调试器 %time 报告某段程序执行时间 %timeit 多次执行某段程序，计算系综平均执行时间，对执行时间非常小的代码很有用。 %xdel variable 删除variable，并清除一切引用。 基于Qt的富GUI控制台安装了PyQt或Pyside，输入ipython qtconsole --pylab=inline这里我的版本是比较新的运行会出现警告，输入juyter qtconsole就行。 matplotlib集成与pylab模式输入ipython --pylab 使用命令历史ipython维护这一个小型的数据库，其中包含你执行过程中每条命令文本。 搜索并重用命令历史按住上或下（或者Ctrl-P或Ctrl-N）你就可以搜索你之前输入过的命令。 输入之前命令的部分字符再按住Crtl-R就能跳出与其匹配的命令。 输入变量和输出变量忘记把函数赋值给变量很让人郁闷，ipython会将一些输入和输出的引用保存在一些特殊的变量里，最近的两个输出结果分别保存在_和__中。 输入文本被保存在名为_ix的变量中，其中x代表行号，对应的输入变量被保存_x中，x的意义一样。exec _ix可以执行命令。%xdel和%reset可以解决释放内存不成功的问题。 与操作系统的交互 %bookmark 使用ipython的目录书签系统 %cd directory 将系统工作目录更改为书签系统 %pwd 返回系统的当前工作目录 %push directory 将当前目录压入堆栈，并转向目标目录 %poped 弹出栈顶目录，并转向目该目录 %dirs 返回一个含有当前目录栈的列表 shell命令和别名以感叹号开头的命令行表示其后的所有内容需要在shell中执行。比如：!ipython可以从ipython中退出然后进入到python标准交互系统中。还可以用%alias 别名 命令来为别名自定义别名，不过这是暂时的。 目录书签系统输入%bookmark db 目录就可以将目录用db来取代，下次直接输入cd db就能进入刚才你输入的目录。 IPython HTML Notebook在命令行中输入jupyter notebook注意这里我的版本，不是以前用的ipython notebook，而且后面也不能随便添加标注–pylab=inline，直接在juyter notebook中输入%pylab inline即可。]]></content>
      <categories>
        <category>python工具</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[module安装问题]]></title>
    <url>%2F2018%2F01%2F29%2Fmodule%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言：由于我为了方便，开始就安装了Anaconda，所以很多模块都已经装好了，遇到没有的模块，我们可以用pip install +模块名字，但是有的时候也会出现问题。 wordcloud安装出错你会发现在cmd的输入命令 pip install wordcloud 时会报错。解决方法： 在以下这个网址http://link.zhihu.com/?target=http%3A//www.lfd.uci.edu/%7Egohlke/pythonlibs/%23lxml下载你需要的的对应版本模块。 接着，把下载下来的文件原封不动的拷贝到你pyhon或者Anaconda的Scripts中，我的就是F:\Anaconda\Scripts 再到cmd中执行pip install F:\Anaconda\Scripts\刚才的文件名 这样就能下载成功！]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>模块安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫（3）]]></title>
    <url>%2F2018%2F01%2F26%2Fpython%E7%88%AC%E8%99%AB%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[之前的两节，我们把我们要的东西爬取下来，但是并没有做存储，下面我们将会讲解如何做存储。 保存到Excel123import pandasdf = pandas.DataFrame(final)df.to_excel('news.xlsx') 这是就可以在我们的目录当中找到news.xlsx的文件。 保存到数据库123import sqlite3with sqlite3.connect('news.sqlite') as db: df.to_sql('news',con = db) 可以利用pandas去sqlite中拿数据：12with sqlite3.connect('news.sqlite') as db: df2 = pandas.read_sql('SELECT * FROM news',con = db) 连上钱两次，就算是一个完整的爬虫过程了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫（2）]]></title>
    <url>%2F2018%2F01%2F26%2F%E7%88%AC%E8%99%AB%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[事前的章节只是负责把新闻的网页爬取下来，对于内文的需求还是不够的，所以这篇文章主要讲的是对于内文信息的爬取，包括标题、时间、作者、来源等等。 还和之前的爬去方法一样，利用Chrome F12就可以找到网页和获取响应的方法，之前有讲过。代码如下：12345import requestsfrom bs4 import BeautifulSoupres = requests.get("http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml")res.encoding = 'utf-8'soup = BeautifulSoup(res.text,'html.parser') 这样就获取了我们所需要的网页部分，继续进行标签的搜寻。 标题。在class=“main-title”的标签之中。 时间。在class=“date-source”中，但是都在span标签下，可以用contents将不同分层的标签下的东西取出。最后用datetime模块将时间字符串变成时间格式。 来源。和上面类似的操作。 内容。找到后会有分段的内容，利用for循环嵌套后就可以取出来，再用join连接成一个完整的部分。 评论数。你在doc中找不到，它在js中，而且还需要用json模块将数据变成我们容易处理的字典。具体代码如下：12345678910111213title = soup.select('.main-title')[0].textdate = soup.select('.date-source')[0].contents[1].textdt = datetime.strptime(date,'%Y年%m月%d日 %H:%M')source = soup.select('.date-source a ')[0].textbody = soup.select('.article p')[:-1]article = '\n'.join([d.text.strip() for d in body])mport jsoncomment = requests.get('http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\channel=gn&amp;newsid=comos-fyqyesy2133291&amp;\group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495')jd = json.loads(comment.text.lstrip('jsonp_1516947697495(').rstrip(')'))jd['result']['count']['total'] 评论数的获取页面链接中包含新闻id，在新闻本身的页面链接能找到。将它独立取出的方法有两个： 用split将链接变成多个块组成的list，然后再用索引取，最后用strip削掉我们不要的部分。 12newurl = 'http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml'newsid = newurl.split('/')[-1].lstrip('doc-i').rstrip('.shtml') 利用正则表达去匹配。 123import re m = re.search('doc-i(.*).shtml',newurl)m.group(1) 当然这是一个页面的评论数，写一个函数就能解决。利用字符串的format把上面获得的字符串填入大家共同的部分中。12345678910111213import re import jsonimport requestscommenturl = 'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\channel=gn&amp;newsid=comos-&#123;&#125;&amp;\group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495'def Getcount(newurl): m = re.search('doc-i(.*).shtml',newurl) newsid = m.group(1) comment = requests.get(commenturl.format(newsid)) jd = json.loads(comment.text.lstrip('jsonp_1516947697495(').rstrip(')')) return jd['result']['count']['total'] 将上面汇总成一个函数中，通过字典返回我们要的所有内文细节：12345678910111213141516171819import requestsfrom bs4 import BeautifulSoupimport re import jsonimport requestsdef News(newurl): article = &#123;&#125; res = requests.get(newurl) res.encoding = 'utf-8' soup = BeautifulSoup(res.text,'html.parser') article['title'] = soup.select('.main-title')[0].text date = soup.select('.date-source')[0].contents[1].text article['dt'] = datetime.strptime(date,'%Y年%m月%d日 %H:%M') article['source'] = soup.select('.date-source a ')[0].text body = soup.select('.article p')[:-1] article['articleBody'] = '\n'.join([d.text.strip() for d in body]) article['author'] = soup.select('.show_author')[0].text.lstrip('责任编辑：') article['commentcount'] = Getcount(newurl) return article 这里定义了一个网页链可以得到的所有内容信息，接下来就是考虑一个页面上的所有网页链接都爬下来。主要是通过在js中找到不同步的分页信息才得以了解所有的网页链接是什么。代码如下：12345678def parserlist(url): newdetails = [] res = requests.get(url) newurlnot = res.text.lstrip(' newsloadercallback(').rstrip(');') jd = json.loads(newurlnot) for d in jd['result']['data']: newdetails.append(News(d['url'])) return newdetails parserlist函数唯一的输入参数就是那个js中的网页连接（管理着一个页面的的所有新闻链接） 但是问题又来了，我们的分页可不止一个，所以我们继续观察分页的网页链接，发现page={数字}表示页数，那么我们可以写一个函数继续扩大工作量。代码如下：12345url = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&amp;cat_1=gnxw&amp;cat_2==gdxw1||=gatxw||=zs-pl||=mtjj&amp;level==1||=2&amp;show_ext=1&amp;show_all=1&amp;show_num=22&amp;tag=1&amp;format=json&amp;page=&#123;&#125;&amp;callback=newsloadercallback&amp;_=1516960730249'final = []for i in range(1,10): newser = parserlist(url.format(i)) final.extend(newser) 最终的final就是我们要的9个页面的所有新闻内文。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫（1）]]></title>
    <url>%2F2018%2F01%2F25%2Fpython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[今天来简单的学习一下爬取网页的文档，然后归档。 第一个简单的爬虫利用chrome F12的检查功能对网页的回应进行读取，这里以新浪网为例。 获取到这些信息后，我们就能够写出一下代码：12345import requestsnew_url = 'http://news.sina.com.cn/society/res = requests.get(new_url)res.encoding = 'utf-8'print(res.text) 打印出来后表明没有问题，这是可以用到BeautifulSoup这个模块，讲刚才打印出来的东西丢进去，交给它处理，注意还需标明剖析器是html.parser。 12from bs4 import BeautifulSoupsoup = BeautifulSoup(res.text,'html.parser') 我们可以知道，在res里有很多的标签，我们需要的文字藏在固定的标签中，所以我们下面要做的就是取出我们需要的标签，需要用到soup的select方法。 12soup2 = soup.select('h2')print(soup2) 结果如下，是一个列表。 但是我们要爬取的内容的标签是以不同的id和class来区分的，这里涉及到css的相关内容，我们暂且不谈。只需要知道我们需要通过网页的检查了解我们的新闻藏在什么杨种类的标签之下。 现在可以知道我们的内容在news-item的class之下，所以我们运用select(‘.news-item’)的方法去获取，同样的方法，我们又可以知道，新闻的标题在h标签下，新闻的时间在值time的class之下，链接在h2标签的href中，写出一下代码。12345678from bs4 import BeautifulSoupsoup = BeautifulSoup(res.text,'html.parser')for link in soup.select('.news-item'): if len(link.select('h2')) &gt; 0: h2 = link.select('h2')[0].text time = link.select('.time')[0].text href = link.select('a')[0]['href'] print(time,h2,href) 注：text取的是标签的文字内容，而[‘href’]取的是标签内部的值。 最终结果如下：]]></content>
  </entry>
  <entry>
    <title><![CDATA[寒假]]></title>
    <url>%2F2018%2F01%2F24%2F%E5%AF%92%E5%81%87%2F</url>
    <content type="text"><![CDATA[我不知道为什么要写这个东西，只是闲着无聊，有不想动笔写日记，反正也没有喜欢的本子，就把闲置的博客拿来用用吧，希望未来自己看到这个能够有所感触。 我在从南京回来的路上听到一则鼓励写作的的录音，说写作并不用很好好的文笔只需要保持一直写就可以了。对于这个观点，我相信，原因很简单，我没有文笔，又喜欢写点东西，决定坚持。 我现在懂了，不是只有难得的事情，做出来了才伟大，简单的事情坚持一直做也是件很了不起的。突然想起来我的高中化学课的老师曾经和我们讲过某企业老板要求自己的员工在每天某个时间点准时起来做一分钟的高抬腿，而且他们真正的做到了。一件事，做一天你可能回说没什么，做一周也还好，一个月很厉害，一年也还好，三年四年。。。你会怎么觉得，我看到一则介绍爱情的短视频，一个男生为自己喜欢的女生每天削一个苹果，坚持了2300天，他们没有吵过架。 从此刻开始，我要为了自己想过的生活儿努力，坚持写作，每天。就和每天吃饭睡觉一样。寒假放假之前，我加了一个学习理财的群，我发现，他们一只都在教我们理财的重要性，大家还在学的那么起劲，我再管网几天，看看又没有干货，有没有的话就退出吧。今天看完了爬虫的一段教程，我觉讲的很有条理，明天继续。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Datastruct]]></title>
    <url>%2F2017%2F12%2F12%2Fdatastruct%2F</url>
    <content type="text"><![CDATA[线性表顺序表示DestroyList是将线性表的所有内容释放。线性表有三个属性，分别是线性表的空间基址、当前内容的长度length以及分配的长度size。（这里分配的长度需要大于内容的长度）DestroyList是讲所有的属性抹去初始值。 ClearList是将当前的内容长度置0。 isEmpty则是通过内容长度来判断线性表是否为空。 GetElem是将第i个元素取出来并返回。 FindElem是将表中元素与查找元素相同的下标输出。 对应的还有插入、删除、访问、历遍等等。。。。。 链式表示链式表示和顺序表示最大的不同就是，链式表示中逻辑位置相邻的数据他们需要物理位置也相邻。插入和删除不需要再将大量的数据做移动，减少算法的复杂度。 定义单链表时会有两个属性，1.数据，2.下一个节点的指针。 作业，编写代码实现链表反转和实现一元多项式的相加。 12/12/2017 11:46:54 AM]]></content>
  </entry>
  <entry>
    <title><![CDATA[考研前的课程复习安排]]></title>
    <url>%2F2017%2F11%2F27%2F%E8%80%83%E7%A0%94%E5%89%8D%E7%9A%84%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0%E5%AE%89%E6%8E%92%2F</url>
    <content type="text"><![CDATA[这学期可能是我大学期间的转折点，也将可能会是我大学阶段面对考试最困难的一个学期，因为我大三了，而且面临考验抉择，所以这个学期志在必得（一门都不许挂），下面是我最低限度的复习计划，针对每天整理出最少的复习量。 这学期的课程分为6门：通原，通线相对可以接受，交换，信息论，单片机，电磁场很难。 现在是第13周，距离考试还有4周不到的时间，我暂且粗略的估计是20天，还留七八天用来巩固。 通原一共13章，一天一章，从明天开始。 通线一共10章，一天一章，从明天开始。 电磁场一共8章，一天一章，从明天开始。 交换机一共11章，一天一章，从12月6日开始。 单片机一共11章，一天一章，从12月7日开始。 #信息论 #一共7章，一天一章，从12月11日开始。 这些课程除外还有一节毛概，一节通创，一节就业指导，以及一节实验。 综合起来将会有10门功课，加油吧，就这么多了。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F11%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
