<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Girlden&#39;s Blog</title>
  
  <subtitle>Quick notes</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-02-25T08:25:14.722Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Girlden</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL数据导入</title>
    <link href="http://yoursite.com/2018/02/25/MySQLL%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5/"/>
    <id>http://yoursite.com/2018/02/25/MySQLL数据导入/</id>
    <published>2018-02-25T07:29:00.000Z</published>
    <updated>2018-02-25T08:25:14.722Z</updated>
    
    <content type="html"><![CDATA[<p>在安装完MySQL数据库后，数据库中的数据呈现最初始的状态，所以需要向其中填充数据，但是这是数据准备阶段的工作。下面简单介绍一下：</p><h1 id="构建数据库和table"><a href="#构建数据库和table" class="headerlink" title="构建数据库和table"></a>构建数据库和table</h1><ol><li>首先需要进入数据库。在管理员命令窗口输入<code>net start mysql5.7.21</code>。</li><li><p>然后再登陆mysql。</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mysql -u root -p</span></span><br></pre></td></tr></table></figure></li><li><p>进入后再创建database。</p><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE test2 <span class="meta">#默认是以utf8编码</span></span><br></pre></td></tr></table></figure></li><li><p>进入test2这个<strong>database中</strong>。</p><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> test2</span><br></pre></td></tr></table></figure></li><li><p>再进行CSV文件的导入。以table的形式。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> TEST_BOOKS1(</span><br><span class="line">      team_name <span class="built_in">varchar</span>(<span class="number">100</span>),</span><br><span class="line">      Wins <span class="built_in">int</span>,</span><br><span class="line">      Draws <span class="built_in">int</span>,</span><br><span class="line">      Losses <span class="built_in">int</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure></li></ol><p>创建table的格式如下：<code>变量名 类型 限制</code><br>以上的实例就是我的表格结构的概括，可以根据自己的需求自行修改参数：<br><img src="https://i.imgur.com/Qb8LePJ.png" alt=""></p><h1 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h1><h2 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h2><p>将自己文件里的数据导入到这个构造好的table中。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LOAD</span> <span class="keyword">DATA</span> <span class="keyword">INFILE</span> <span class="string">'C:\Users\diannao\Desktop\2015-16-premier-league.csv'</span></span><br><span class="line"><span class="keyword">INTO</span> <span class="keyword">TABLE</span> TEST_BOOKS1</span><br><span class="line"><span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">','</span></span><br><span class="line"><span class="keyword">ENCLOSED</span> <span class="keyword">BY</span> <span class="string">'"'</span></span><br><span class="line"><span class="keyword">LINES</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\n'</span></span><br><span class="line">IGONRE <span class="number">1</span> <span class="keyword">ROWS</span>;</span><br></pre></td></tr></table></figure></p><h2 id="错误解决"><a href="#错误解决" class="headerlink" title="错误解决"></a>错误解决</h2><p>这里需要提一下，如果运行不成功，并且显示的错误是secure-file-priv有问题，那么就需要在my.ini文件中的[mysqld]下进行声明<code>secure-file-priv=</code>,这里的意思就是对于导入和导出不去进行干扰。注意，声明完成后，还需要在管理员命令窗口下对mysql5.7.21进行重启，才能有效。</p><h2 id="检验"><a href="#检验" class="headerlink" title="检验"></a>检验</h2><p>检验的方法<code>SELECT * FROM TEST_BOOKS1</code><br><img src="https://i.imgur.com/YaAxHeD.png" alt=""><br>到这里就算导入成功了！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在安装完MySQL数据库后，数据库中的数据呈现最初始的状态，所以需要向其中填充数据，但是这是数据准备阶段的工作。下面简单介绍一下：&lt;/p&gt;
&lt;h1 id=&quot;构建数据库和table&quot;&gt;&lt;a href=&quot;#构建数据库和table&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
      <category term="数据准备" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87/"/>
    
  </entry>
  
  <entry>
    <title>MySQL安装与配置</title>
    <link href="http://yoursite.com/2018/02/24/MySQL%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2018/02/24/MySQL安装/</id>
    <published>2018-02-24T11:47:01.000Z</published>
    <updated>2018-02-25T02:51:15.368Z</updated>
    
    <content type="html"><![CDATA[<p>我们首先可以在<a href="https://dev.mysql.com/downloads/mysql/" target="_blank" rel="noopener">Mysql的官网</a>上找到我们的需要下载的文件。选择符合自己电脑的版本。</p><p><img src="https://i.imgur.com/rXpNNFo.png" alt=""></p><p>这些都是不需要安装的压缩包，这里就讲一下怎么用这个压缩包安装。</p><ol><li>解压文件，在文件里添加初始化文件<strong>my.ini</strong>。先新建txt文档然后再更名为my.ini。<br><img src="https://i.imgur.com/i9ktpSH.png" alt=""><br>在此文档里添加一段初始化的代码。<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line"><span class="comment">#设置mysql客户端默认字符集</span></span><br><span class="line"><span class="attribute">default-character-set</span>=utf8</span><br><span class="line">[mysqld]</span><br><span class="line"><span class="comment">#设置3306端口</span></span><br><span class="line"><span class="attribute">port</span>=3306</span><br><span class="line"><span class="comment">#设置mysql的安装目录</span></span><br><span class="line"><span class="attribute">basedir</span>=D:\Program Files\mysql-5.7.21-winx64</span><br><span class="line"><span class="comment">#设置mysql数据库的数据库的存放目录</span></span><br><span class="line"><span class="attribute">datadir</span>=D:\Program Files\mysql-5.7.21-winx64\data</span><br><span class="line"><span class="comment">#允许最大连接数</span></span><br><span class="line">max_connections = 200</span><br><span class="line"><span class="comment">#服务端使用的字符集默认为8比特的latin1字符集</span></span><br><span class="line"><span class="attribute">character-set-server</span>=utf8</span><br><span class="line"><span class="comment">#开启查询缓存</span></span><br><span class="line">explicit_defaults_for_timestamp = <span class="literal">true</span></span><br><span class="line">skip-grant-tables</span><br><span class="line"><span class="comment">#创建新表将使用的默认存储引擎</span></span><br><span class="line"><span class="attribute">default-storage-engine</span>=INNODB</span><br></pre></td></tr></table></figure></li></ol><p>需要注意的是：上面第二个【】里是mysqld。</p><ol><li><p>配置系统变量。在系统变量里添加MYSQL_HOME,路径填你解压文件所在的目录下，我的是<strong>D:\Program Files\mysql-5.7.21-winx64</strong>。然后在系统path里添加：<strong>%MYSQL_HOME%\bin</strong>。</p></li><li><p>以管理员身份运行命令终端cmd。执行一下代码：</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d:</span><br><span class="line"><span class="keyword">cd</span> *D:\Program Files\mysql-5.7.21-winx64\bin</span><br><span class="line">mysqld <span class="params">--initialize</span> <span class="params">--user=mysql</span> <span class="params">--console</span></span><br></pre></td></tr></table></figure></li><li><p>安装服务。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">mysqld</span> <span class="selector-tag">--install</span> <span class="selector-tag">MySQL5</span><span class="selector-class">.7</span><span class="selector-class">.21</span></span><br></pre></td></tr></table></figure></li><li><p>启动服务。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">net</span> <span class="selector-tag">start</span> <span class="selector-tag">MySQL5</span><span class="selector-class">.7</span><span class="selector-class">.21</span></span><br></pre></td></tr></table></figure></li><li><p>修改密码。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">password</span> <span class="keyword">for</span> root@<span class="keyword">local</span>=<span class="keyword">password</span>(<span class="string">'你的密码'</span>)</span><br></pre></td></tr></table></figure></li></ol><p>每次登陆mysql，就输入<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">mysql -u root -p</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们首先可以在&lt;a href=&quot;https://dev.mysql.com/downloads/mysql/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mysql的官网&lt;/a&gt;上找到我们的需要下载的文件。选择符合自己电脑的版本。&lt;/p&gt;
&lt;p&gt;&lt;img
      
    
    </summary>
    
      <category term="数据库" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="MySQL" scheme="http://yoursite.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>利用Python进行数据分析之Pandas</title>
    <link href="http://yoursite.com/2018/02/21/%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8BPandas/"/>
    <id>http://yoursite.com/2018/02/21/利用Python进行数据分析之Pandas/</id>
    <published>2018-02-21T06:19:56.000Z</published>
    <updated>2018-02-21T10:40:33.319Z</updated>
    
    <content type="html"><![CDATA[<p>2008年开始就有pandas库，构建者构造它的原因就是没有任何一个工具可以满足他的工作需求。</p><ul><li>具备按轴自动或显示数据的对齐的功能的数据结构。</li><li>集成时间序列功能。</li><li>既能处理时间序列数据也能处理非时间序列数据的数据结构。</li><li>数据运算和约简可以根据不同的元素数据执行。</li><li>灵活处理缺失数据</li><li>合并及其他出现在常见数据库中的关系型运算。</li></ul><p>我</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2008年开始就有pandas库，构建者构造它的原因就是没有任何一个工具可以满足他的工作需求。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;具备按轴自动或显示数据的对齐的功能的数据结构。&lt;/li&gt;
&lt;li&gt;集成时间序列功能。&lt;/li&gt;
&lt;li&gt;既能处理时间序列数据也能处理非时间序列数据的数据
      
    
    </summary>
    
      <category term="python工具" scheme="http://yoursite.com/categories/python%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Pandas" scheme="http://yoursite.com/tags/Pandas/"/>
    
  </entry>
  
  <entry>
    <title>利用Python进行数据分析之IPython</title>
    <link href="http://yoursite.com/2018/02/20/%E5%88%A9%E7%94%A8Python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2018/02/20/利用Python进行数据分析/</id>
    <published>2018-02-20T06:58:03.000Z</published>
    <updated>2018-02-21T06:23:58.561Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ipython"><a href="#ipython" class="headerlink" title="ipython"></a>ipython</h1><p><strong>ipython外加一个文本编辑器</strong>是较好的Python开发环境。当然，一款IDE也是不错的选择，但是有些IDE本身就是集成了IPython。</p><p>IPython本身并没有提供任何的计算和数据分析的功能，其主要目地就是在交互式计算和软件开发这两个方面最大化的提高生产力。他鼓励的是“<strong>执行 探索</strong>”的工作模式，而不是许多变成语言那种“编辑 编译 运行”的传统模式。</p><h2 id="python基础"><a href="#python基础" class="headerlink" title="python基础"></a>python基础</h2><p>启动就像标准的Python解释器那样输入ipython即可进入。<br><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">C:\Users\diannao</span><br><span class="line">λ ipython</span><br><span class="line">Python <span class="number">3.6</span>.<span class="number">3</span> |Anaconda custom (<span class="number">64</span>-<span class="built_in">bit</span>)| (<span class="keyword">default</span>, Oct <span class="number">15</span> <span class="number">2017</span>, <span class="number">03</span>:<span class="number">27</span>:<span class="number">45</span>) [MSC v.<span class="number">1900</span> <span class="number">64</span> <span class="built_in">bit</span> (AMD64)]</span><br><span class="line"><span class="keyword">Type</span> <span class="symbol">'copyright</span>', <span class="symbol">'credits</span>' <span class="keyword">or</span> <span class="symbol">'license</span>' <span class="keyword">for</span> more information</span><br><span class="line">IPython <span class="number">6.1</span>.<span class="number">0</span> <span class="comment">-- An enhanced Interactive Python. Type '?' for help.</span></span><br></pre></td></tr></table></figure></p><p>注意的是Python对象都被格式化的可读性很好，和print的输出形式有着显著的区别。</p><p><img src="https://i.imgur.com/U3bmXhW.png" alt=""></p><p><img src="https://i.imgur.com/4eDeIEw.png" alt=""></p><h2 id="Tab键自动填充"><a href="#Tab键自动填充" class="headerlink" title="Tab键自动填充"></a>Tab键自动填充</h2><p>在shell中输入表达式时，只要按下Tab键，当前命名空间只要有与输入的字符串相匹配的变量就会被找出：<br><img src="https://i.imgur.com/nSXyZRI.png" alt=""><br>而且还可以用在任何对象后面以及模块后面：<br><img src="https://i.imgur.com/Uw9aKTM.png" alt=""></p><p>据说还可以填充路径字符串，本人无法成功，原因不明。</p><h2 id="内省"><a href="#内省" class="headerlink" title="内省"></a>内省</h2><p>可以显示命名空间的信对象信息，以及函数的信息（一个问号会显示docstring，连个问号会显示整个函数的源代码）。<br><img src="https://i.imgur.com/Ne6uMej.png" alt=""></p><p><img src="https://i.imgur.com/Ysabq37.png" alt=""></p><p><img src="https://i.imgur.com/mvJskMN.png" alt=""></p><p>还可以作为正则表达式中去匹配：</p><p><img src="https://i.imgur.com/FuruaTS.png" alt=""></p><h2 id="run命令"><a href="#run命令" class="headerlink" title="%run命令"></a>%run命令</h2><p>用来执行脚本文件。<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%<span class="keyword">run</span><span class="bash"> 文件名.py</span></span><br></pre></td></tr></table></figure></p><p>执行之后就可以在ipython shell中运用脚本里的全局变量、函数、模块等等。</p><h2 id="执行剪切板中的代码"><a href="#执行剪切板中的代码" class="headerlink" title="执行剪切板中的代码"></a>执行剪切板中的代码</h2><p>利用%paste和%cpaste。%paste是直接复制后立即执行，%cpaste可以对此复制，方法是键入%cpaste后再点击右键会有提示，不停的在剪切板中添加新的元素就可以不断的复制粘贴，最后输入–，如果发现不对就输入Ctrl-c。</p><h2 id="键盘快捷键"><a href="#键盘快捷键" class="headerlink" title="键盘快捷键"></a>键盘快捷键</h2><ul><li>Ctrl-A  将光标移动到行首</li><li>Ctrl-E  将光标移动到行尾</li><li>Ctrl-K  删除从光标开始至行尾的文本</li><li>Ctrl-U  清除当前行的所有文本</li><li>Ctrl-L  清屏</li></ul><h2 id="魔术命令"><a href="#魔术命令" class="headerlink" title="魔术命令"></a>魔术命令</h2><ul><li>%quickref  显示IPython的快速参考</li><li>%magic     显示所有魔术命令的详细文档</li><li>%debug     从最新的异常跟踪的底端进入交互式调试</li><li>%hist      打印命令的输入历史</li><li>%pdb       在异常发生后自动进入调试器</li><li>%time      报告某段程序执行时间</li><li>%timeit    多次执行某段程序，计算系综平均执行时间，对执行时间非常小的代码很有用。</li><li>%xdel variable 删除variable，并清除一切引用。</li></ul><h2 id="基于Qt的富GUI控制台"><a href="#基于Qt的富GUI控制台" class="headerlink" title="基于Qt的富GUI控制台"></a>基于Qt的富GUI控制台</h2><p>安装了PyQt或Pyside，输入<code>ipython qtconsole --pylab=inline</code><br>这里我的版本是比较新的运行会出现警告，输入<code>juyter qtconsole</code>就行。</p><h2 id="matplotlib集成与pylab模式"><a href="#matplotlib集成与pylab模式" class="headerlink" title="matplotlib集成与pylab模式"></a>matplotlib集成与pylab模式</h2><p>输入<code>ipython --pylab</code></p><h2 id="使用命令历史"><a href="#使用命令历史" class="headerlink" title="使用命令历史"></a>使用命令历史</h2><p>ipython维护这一个小型的数据库，其中包含你执行过程中每条命令文本。</p><h3 id="搜索并重用命令历史"><a href="#搜索并重用命令历史" class="headerlink" title="搜索并重用命令历史"></a>搜索并重用命令历史</h3><p>按住上或下（或者Ctrl-P或Ctrl-N）你就可以搜索你之前输入过的命令。</p><p>输入之前命令的部分字符再按住Crtl-R就能跳出与其匹配的命令。</p><h3 id="输入变量和输出变量"><a href="#输入变量和输出变量" class="headerlink" title="输入变量和输出变量"></a>输入变量和输出变量</h3><p>忘记把函数赋值给变量很让人郁闷，ipython会将一些输入和输出的引用保存在一些特殊的变量里，<strong>最近的两个输出</strong>结果分别保存在_和__中。</p><p>输入文本被保存在名为_ix的变量中，其中x代表行号，<br>对应的输入变量被保存_x中，x的意义一样。<br><code>exec _ix</code>可以执行命令。<br><code>%xdel</code>和<code>%reset</code>可以解决释放内存不成功的问题。</p><h2 id="与操作系统的交互"><a href="#与操作系统的交互" class="headerlink" title="与操作系统的交互"></a>与操作系统的交互</h2><ul><li>%bookmark        使用ipython的目录书签系统</li><li>%cd directory    将系统工作目录更改为书签系统</li><li>%pwd             返回系统的当前工作目录</li><li>%push directory  将当前目录压入堆栈，并转向目标目录</li><li>%poped           弹出栈顶目录，并转向目该目录</li><li>%dirs            返回一个含有当前目录栈的列表</li></ul><h3 id="shell命令和别名"><a href="#shell命令和别名" class="headerlink" title="shell命令和别名"></a>shell命令和别名</h3><p>以感叹号开头的命令行表示其后的所有内容需要在shell中执行。比如：<br><code>!ipython</code>可以从ipython中退出然后进入到python标准交互系统中。<br>还可以用<code>%alias 别名 命令</code>来为别名自定义别名，不过这是暂时的。</p><h3 id="目录书签系统"><a href="#目录书签系统" class="headerlink" title="目录书签系统"></a>目录书签系统</h3><p>输入<code>%bookmark db 目录</code>就可以将目录用db来取代，下次直接输入<code>cd db</code>就能进入刚才你输入的目录。</p><h2 id="IPython-HTML-Notebook"><a href="#IPython-HTML-Notebook" class="headerlink" title="IPython HTML Notebook"></a>IPython HTML Notebook</h2><p>在命令行中输入<code>jupyter notebook</code><br>注意这里我的版本，不是以前用的ipython notebook，而且后面也不能随便添加标注–<strong>pylab=inline</strong>，直接在<strong>juyter notebook</strong>中输入%<strong>pylab inline</strong>即可。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ipython&quot;&gt;&lt;a href=&quot;#ipython&quot; class=&quot;headerlink&quot; title=&quot;ipython&quot;&gt;&lt;/a&gt;ipython&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;ipython外加一个文本编辑器&lt;/strong&gt;是较好的Python开发环境。当然
      
    
    </summary>
    
      <category term="python工具" scheme="http://yoursite.com/categories/python%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="数据分析" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>module安装问题</title>
    <link href="http://yoursite.com/2018/01/29/module%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/01/29/module安装问题/</id>
    <published>2018-01-29T06:00:39.000Z</published>
    <updated>2018-01-29T06:21:39.215Z</updated>
    
    <content type="html"><![CDATA[<p>前言：由于我为了方便，开始就安装了Anaconda，所以很多模块都已经装好了，遇到没有的模块，我们可以用pip install +模块名字，但是有的时候也会出现问题。</p><h1 id="wordcloud安装出错"><a href="#wordcloud安装出错" class="headerlink" title="wordcloud安装出错"></a>wordcloud安装出错</h1><p>你会发现在cmd的输入命令 <code>pip install wordcloud</code> 时会报错。<br><strong>解决方法</strong>：</p><p>在以下这个网址<br><a href="http://link.zhihu.com/?target=http%3A//www.lfd.uci.edu/%7Egohlke/pythonlibs/%23lxml" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//www.lfd.uci.edu/%7Egohlke/pythonlibs/%23lxml</a><br>下载你需要的的对应版本模块。</p><p><img src="https://i.imgur.com/WIuFEGl.png" alt=""></p><p>接着，把下载下来的文件原封不动的拷贝到你pyhon或者Anaconda的Scripts中，我的就是F:\Anaconda\Scripts</p><p>再到cmd中执行pip install F:\Anaconda\Scripts\刚才的文件名</p><p>这样就能下载成功！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前言：由于我为了方便，开始就安装了Anaconda，所以很多模块都已经装好了，遇到没有的模块，我们可以用pip install +模块名字，但是有的时候也会出现问题。&lt;/p&gt;
&lt;h1 id=&quot;wordcloud安装出错&quot;&gt;&lt;a href=&quot;#wordcloud安装出错&quot; c
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="模块安装" scheme="http://yoursite.com/tags/%E6%A8%A1%E5%9D%97%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫（3）</title>
    <link href="http://yoursite.com/2018/01/26/python%E7%88%AC%E8%99%AB%EF%BC%883%EF%BC%89/"/>
    <id>http://yoursite.com/2018/01/26/python爬虫（3）/</id>
    <published>2018-01-26T14:25:49.000Z</published>
    <updated>2018-01-26T14:57:49.291Z</updated>
    
    <content type="html"><![CDATA[<p>之前的两节，我们把我们要的东西爬取下来，但是并没有做存储，下面我们将会讲解如何做存储。</p><h2 id="保存到Excel"><a href="#保存到Excel" class="headerlink" title="保存到Excel"></a>保存到Excel</h2><figure class="highlight aspectj"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">df = pandas.DataFrame(<span class="keyword">final</span>)</span><br><span class="line">df.to_excel(<span class="string">'news.xlsx'</span>)</span><br></pre></td></tr></table></figure><p>这是就可以在我们的目录当中找到news.xlsx的文件。</p><h2 id="保存到数据库"><a href="#保存到数据库" class="headerlink" title="保存到数据库"></a>保存到数据库</h2><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sqlite3</span><br><span class="line"><span class="keyword">with</span> sqlite3.connect(<span class="string">'news.sqlite'</span>) <span class="keyword">as</span> db:</span><br><span class="line">    df.to_sql(<span class="string">'news'</span>,con = db)</span><br></pre></td></tr></table></figure><p>可以利用pandas去sqlite中拿数据：<br><figure class="highlight sml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> sqlite3.connect(<span class="symbol">'news</span>.sqlite') <span class="keyword">as</span> db:</span><br><span class="line">    df2 = pandas.read_sql(<span class="symbol">'SELECT</span> * <span class="type">FROM</span> news',con = db)</span><br></pre></td></tr></table></figure></p><p>连上钱两次，就算是一个完整的爬虫过程了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前的两节，我们把我们要的东西爬取下来，但是并没有做存储，下面我们将会讲解如何做存储。&lt;/p&gt;
&lt;h2 id=&quot;保存到Excel&quot;&gt;&lt;a href=&quot;#保存到Excel&quot; class=&quot;headerlink&quot; title=&quot;保存到Excel&quot;&gt;&lt;/a&gt;保存到Excel&lt;/h
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="数据库" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫（2）</title>
    <link href="http://yoursite.com/2018/01/26/%E7%88%AC%E8%99%AB%EF%BC%882%EF%BC%89/"/>
    <id>http://yoursite.com/2018/01/26/爬虫（2）/</id>
    <published>2018-01-26T03:48:14.000Z</published>
    <updated>2018-01-26T14:16:19.019Z</updated>
    
    <content type="html"><![CDATA[<p>事前的章节只是负责把新闻的网页爬取下来，对于内文的需求还是不够的，所以这篇文章主要讲的是对于内文信息的爬取，包括<strong>标题、时间、作者、来源</strong>等等。</p><p>还和之前的爬去方法一样，利用<strong>Chrome F12</strong>就可以找到网页和获取响应的方法，之前有讲过。代码如下：<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">res = requests.get(<span class="string">"http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml"</span>)</span><br><span class="line">res.encoding = <span class="string">'utf-8'</span></span><br><span class="line">soup = BeautifulSoup(res.<span class="keyword">text</span>,<span class="string">'html.parser'</span>)</span><br></pre></td></tr></table></figure></p><p>这样就获取了我们所需要的网页部分，继续进行标签的搜寻。</p><ol><li>标题。在class=“main-title”的标签之中。</li><li>时间。在class=“date-source”中，但是都在span标签下，可以用contents将不同分层的标签下的东西取出。最后用datetime模块将<strong>时间字符串</strong>变成<strong>时间格式</strong>。</li><li>来源。和上面类似的操作。</li><li>内容。找到后会有分段的内容，利用<strong>for循环</strong>嵌套后就可以取出来，再用<strong>join</strong>连接成一个完整的部分。</li><li>评论数。你在doc中找不到，它在js中，而且还需要用json模块将数据变成我们容易处理的字典。<br>具体代码如下：<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">title = soup.select(<span class="string">'.main-title'</span>)[<span class="number">0</span>].text</span><br><span class="line">date = soup.select(<span class="string">'.date-source'</span>)[<span class="number">0</span>]<span class="selector-class">.contents</span>[<span class="number">1</span>].text</span><br><span class="line"><span class="selector-tag">dt</span> = datetime.strptime(date,<span class="string">'%Y年%m月%d日 %H:%M'</span>)</span><br><span class="line">source = soup.select(<span class="string">'.date-source a '</span>)[<span class="number">0</span>].text</span><br><span class="line"><span class="selector-tag">body</span> = soup.select(<span class="string">'.article p'</span>)[:-<span class="number">1</span>]</span><br><span class="line"><span class="selector-tag">article</span> = <span class="string">'\n'</span>.join([d<span class="selector-class">.text</span><span class="selector-class">.strip</span>() <span class="keyword">for</span> d <span class="keyword">in</span> body])</span><br><span class="line">mport json</span><br><span class="line">comment = requests.get(<span class="string">'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\</span></span><br><span class="line"><span class="string">channel=gn&amp;newsid=comos-fyqyesy2133291&amp;\</span></span><br><span class="line"><span class="string">group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\</span></span><br><span class="line"><span class="string">h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495'</span>)</span><br><span class="line">jd = json.loads(comment<span class="selector-class">.text</span><span class="selector-class">.lstrip</span>(<span class="string">'jsonp_1516947697495('</span>).rstrip(<span class="string">')'</span>))</span><br><span class="line">jd[<span class="string">'result'</span>][<span class="string">'count'</span>][<span class="string">'total'</span>]</span><br></pre></td></tr></table></figure></li></ol><p>评论数的获取页面链接中包含新闻<strong>id</strong>，在新闻本身的页面链接能找到。将它独立取出的方法有两个：</p><ol><li><p>用<strong>split</strong>将链接变成多个块组成的list，然后再用索引取，最后用<strong>strip</strong>削掉我们不要的部分。</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span><span class="type">url</span> = <span class="string">'http://news.sina.com.cn/o/2018-01-26/doc-ifyqyesy2133291.shtml'</span></span><br><span class="line"><span class="keyword">new</span><span class="type">sid</span> = <span class="keyword">new</span><span class="type">url</span>.split(<span class="string">'/'</span>)[<span class="number">-1</span>].lstrip(<span class="string">'doc-i'</span>).rstrip(<span class="string">'.shtml'</span>)</span><br></pre></td></tr></table></figure></li><li><p>利用正则表达去匹配。</p><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line">m = re.search(<span class="string">'doc-i(.*).shtml'</span>,<span class="keyword">new</span><span class="type">url</span>)</span><br><span class="line">m.group(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ol><p>当然这是一个页面的评论数，写一个函数就能解决。利用<strong>字符串的format</strong>把上面获得的字符串填入大家共同的部分中。<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">commenturl = <span class="string">'http://comment5.news.sina.com.cn/page/info?version=1&amp;format=json&amp;\</span></span><br><span class="line"><span class="string">channel=gn&amp;newsid=comos-&#123;&#125;&amp;\</span></span><br><span class="line"><span class="string">group=undefined&amp;compress=0&amp;ie=utf-8&amp;oe=utf-8&amp;page=1&amp;page_size=3&amp;t_size=3&amp;\</span></span><br><span class="line"><span class="string">h_size=3&amp;thread=1&amp;callback=jsonp_1516947697495&amp;_=1516947697495'</span></span><br><span class="line">def Getcount(<span class="keyword">new</span><span class="type">url</span>):<span class="type"></span></span><br><span class="line"><span class="type">    m </span>= re.search(<span class="string">'doc-i(.*).shtml'</span>,<span class="keyword">new</span><span class="type">url</span>)</span><br><span class="line">    <span class="keyword">new</span><span class="type">sid</span> = m.group(<span class="number">1</span>)</span><br><span class="line">    comment = requests.<span class="keyword">get</span>(commenturl.format(<span class="keyword">new</span><span class="type">sid</span>))</span><br><span class="line">    jd = json.loads(comment.text.lstrip(<span class="string">'jsonp_1516947697495('</span>).rstrip(<span class="string">')'</span>))</span><br><span class="line">    <span class="keyword">return</span> jd[<span class="string">'result'</span>][<span class="string">'count'</span>][<span class="string">'total'</span>]</span><br></pre></td></tr></table></figure></p><p>将上面汇总成一个函数中，通过字典返回我们要的所有内文细节：<br><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">import</span> re </span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">def News(newurl):</span><br><span class="line">    article = &#123;&#125;</span><br><span class="line">    res = requests.get(newurl)</span><br><span class="line">    res.encoding = <span class="string">'utf-8'</span></span><br><span class="line">    soup = BeautifulSoup(res.<span class="keyword">text</span>,<span class="string">'html.parser'</span>)</span><br><span class="line">    article[<span class="string">'title'</span>] = soup.select(<span class="string">'.main-title'</span>)[<span class="number">0</span>].<span class="keyword">text</span></span><br><span class="line">    date = soup.select(<span class="string">'.date-source'</span>)[<span class="number">0</span>].<span class="built_in">contents</span>[<span class="number">1</span>].<span class="keyword">text</span></span><br><span class="line">    article[<span class="string">'dt'</span>] = datetime.strptime(date,<span class="string">'%Y年%m月%d日 %H:%M'</span>)</span><br><span class="line">    article[<span class="string">'source'</span>] = soup.select(<span class="string">'.date-source a '</span>)[<span class="number">0</span>].<span class="keyword">text</span></span><br><span class="line">    body = soup.select(<span class="string">'.article p'</span>)[:-<span class="number">1</span>]</span><br><span class="line">    article[<span class="string">'articleBody'</span>] = <span class="string">'\n'</span>.join([d.<span class="keyword">text</span>.strip() <span class="keyword">for</span> d <span class="built_in">in</span> body])</span><br><span class="line">    article[<span class="string">'author'</span>] = soup.select(<span class="string">'.show_author'</span>)[<span class="number">0</span>].<span class="keyword">text</span>.lstrip(<span class="string">'责任编辑：'</span>)</span><br><span class="line">    article[<span class="string">'commentcount'</span>] = Getcount(newurl)</span><br><span class="line">    return article</span><br></pre></td></tr></table></figure></p><p>这里定义了一个网页链可以得到的所有内容信息，接下来就是考虑一个页面上的所有网页链接都爬下来。主要是通过在<strong>js中找到不同步的分页信息</strong>才得以了解所有的网页链接是什么。代码如下：<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def parserlist(url): <span class="type"></span></span><br><span class="line"><span class="type">    newdetails </span>= []</span><br><span class="line">    res = requests.<span class="keyword">get</span>(url)</span><br><span class="line">    <span class="keyword">new</span><span class="type">urlnot</span> = res.text.lstrip(<span class="string">'  newsloadercallback('</span>).rstrip(<span class="string">');'</span>)</span><br><span class="line">    jd = json.loads(<span class="keyword">new</span><span class="type">urlnot</span>)</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> jd[<span class="string">'result'</span>][<span class="string">'data'</span>]:<span class="type"></span></span><br><span class="line"><span class="type">        newdetails</span>.append(News(d[<span class="string">'url'</span>]))</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span><span class="type">details</span></span><br></pre></td></tr></table></figure></p><p><strong>parserlist</strong>函数唯一的输入参数就是那个<strong>js</strong>中的网页连接（管理着一个页面的的所有新闻链接）</p><p>但是问题又来了，我们的分页可不止一个，所以我们继续观察分页的网页链接，发现page={数字}表示页数，那么我们可以写一个函数继续扩大工作量。代码如下：<br><figure class="highlight coq"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">url = 'http://api.roll.news.sina.com.cn/zt_list?channel=news&amp;cat_1=gnxw&amp;cat_2==gdxw1|<span class="type">|=gatxw</span>|<span class="type">|=zs</span>-pl|<span class="type">|=mtjj</span>&amp;level==<span class="number">1</span>|<span class="type">|=2</span>&amp;show_ext=<span class="number">1</span>&amp;show_all=<span class="number">1</span>&amp;show_num=<span class="number">22</span>&amp;tag=<span class="number">1</span>&amp;format=json&amp;page=&#123;&#125;&amp;callback=newsloadercallback&amp;<span class="keyword">_</span>=<span class="number">1516960730249</span>'</span><br><span class="line">final = []</span><br><span class="line"><span class="keyword">for</span> i <span class="built_in">in</span> range(<span class="number">1</span>,<span class="number">10</span>):</span><br><span class="line">    newser = parserlist(url.format(i))</span><br><span class="line">    final.extend(newser)</span><br></pre></td></tr></table></figure></p><p>最终的<strong>final</strong>就是我们要的9个页面的所有新闻内文。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;事前的章节只是负责把新闻的网页爬取下来，对于内文的需求还是不够的，所以这篇文章主要讲的是对于内文信息的爬取，包括&lt;strong&gt;标题、时间、作者、来源&lt;/strong&gt;等等。&lt;/p&gt;
&lt;p&gt;还和之前的爬去方法一样，利用&lt;strong&gt;Chrome F12&lt;/strong&gt;就
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="爬虫" scheme="http://yoursite.com/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python爬虫（1）</title>
    <link href="http://yoursite.com/2018/01/25/python%E7%88%AC%E8%99%AB/"/>
    <id>http://yoursite.com/2018/01/25/python爬虫/</id>
    <published>2018-01-25T04:07:52.000Z</published>
    <updated>2018-01-25T16:51:07.584Z</updated>
    
    <content type="html"><![CDATA[<p>今天来简单的学习一下爬取网页的文档，然后归档。</p><h1 id="第一个简单的爬虫"><a href="#第一个简单的爬虫" class="headerlink" title="第一个简单的爬虫"></a>第一个简单的爬虫</h1><p>利用<strong>chrome F12</strong>的检查功能对网页的回应进行读取，这里以新浪网为例。</p><p><img src="https://i.imgur.com/HIf2TXK.png" alt=""></p><p><img src="https://i.imgur.com/5xFFPul.png" alt=""></p><p>获取到这些信息后，我们就能够写出一下代码：<br><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">new_url = 'http:<span class="comment">//news.sina.com.cn/society/</span></span><br><span class="line">res = requests.<span class="keyword">get</span>(new_url)</span><br><span class="line">res.encoding = 'utf-<span class="number">8</span>'</span><br><span class="line"><span class="built_in">print</span>(res.text)</span><br></pre></td></tr></table></figure></p><p>打印出来后表明没有问题，这是可以用到BeautifulSoup这个模块，讲刚才打印出来的东西丢进去，交给它处理，注意还需标明剖析器是html.parser。</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(res.<span class="keyword">text</span>,<span class="string">'html.parser'</span>)</span><br></pre></td></tr></table></figure><p>我们可以知道，在res里有很多的标签，我们需要的文字藏在固定的标签中，所以我们下面要做的就是取出我们需要的标签，需要用到soup的select方法。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup2 = soup.select(<span class="string">'h2'</span>)</span><br><span class="line"><span class="function"><span class="title">print</span><span class="params">(soup2)</span></span></span><br></pre></td></tr></table></figure><p>结果如下，是一个列表。<br><img src="https://i.imgur.com/nYTzwP3.png" alt=""></p><p>但是我们要爬取的内容的标签是以不同的id和class来区分的，这里涉及到css的相关内容，我们暂且不谈。只需要知道我们需要通过网页的<strong>检查</strong>了解我们的新闻藏在什么杨种类的标签之下。</p><p><img src="https://i.imgur.com/W7x0Sz2.png" alt=""></p><p>现在可以知道我们的内容在news-item的class之下，所以我们运用select(‘.news-item’)的方法去获取，同样的方法，我们又可以知道，新闻的标题在<strong>h标签下</strong>，新闻的时间在<strong>值time的class之下</strong>，链接在<strong>h2标签的href中</strong>，写出一下代码。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">soup = BeautifulSoup(res<span class="selector-class">.text</span>,<span class="string">'html.parser'</span>)</span><br><span class="line"><span class="keyword">for</span> link <span class="keyword">in</span> soup.select(<span class="string">'.news-item'</span>):</span><br><span class="line">    <span class="keyword">if</span> len(link.select(<span class="string">'h2'</span>)) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="selector-tag">h2</span> = link.select(<span class="string">'h2'</span>)[<span class="number">0</span>].text</span><br><span class="line">        <span class="selector-tag">time</span> = link.select(<span class="string">'.time'</span>)[<span class="number">0</span>].text</span><br><span class="line">        href = link.select(<span class="string">'a'</span>)[<span class="number">0</span>][<span class="string">'href'</span>]</span><br><span class="line">        print(<span class="selector-tag">time</span>,<span class="selector-tag">h2</span>,href)</span><br></pre></td></tr></table></figure></p><p><strong>注：text取的是标签的文字内容，而[‘href’]取的是标签内部的值。</strong></p><p>最终结果如下：</p><p><img src="https://i.imgur.com/pjSMwDJ.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天来简单的学习一下爬取网页的文档，然后归档。&lt;/p&gt;
&lt;h1 id=&quot;第一个简单的爬虫&quot;&gt;&lt;a href=&quot;#第一个简单的爬虫&quot; class=&quot;headerlink&quot; title=&quot;第一个简单的爬虫&quot;&gt;&lt;/a&gt;第一个简单的爬虫&lt;/h1&gt;&lt;p&gt;利用&lt;strong&gt;chrom
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>寒假</title>
    <link href="http://yoursite.com/2018/01/24/%E5%AF%92%E5%81%87/"/>
    <id>http://yoursite.com/2018/01/24/寒假/</id>
    <published>2018-01-23T16:23:03.000Z</published>
    <updated>2018-01-25T16:51:21.557Z</updated>
    
    <content type="html"><![CDATA[<p>我不知道为什么要写这个东西，只是闲着无聊，有不想动笔写日记，反正也没有喜欢的本子，就把闲置的博客拿来用用吧，希望未来自己看到这个能够有所感触。</p><p>我在从南京回来的路上听到一则鼓励写作的的录音，说写作并不用很好好的文笔只需要保持一直写就可以了。对于这个观点，我相信，原因很简单，我没有文笔，又喜欢写点东西，决定坚持。</p><p>我现在懂了，不是只有难得的事情，做出来了才伟大，简单的事情坚持一直做也是件很了不起的。突然想起来我的高中化学课的老师曾经和我们讲过某企业老板要求自己的员工在每天某个时间点准时起来做一分钟的高抬腿，而且他们真正的做到了。一件事，做一天你可能回说没什么，做一周也还好，一个月很厉害，一年也还好，三年四年。。。你会怎么觉得，我看到一则介绍爱情的短视频，<strong>一个男生为自己喜欢的女生每天削一个苹果，坚持了2300天</strong>，他们没有吵过架。</p><p>从此刻开始，我要为了自己想过的生活儿努力，坚持写作，每天。就和每天吃饭睡觉一样。寒假放假之前，我加了一个学习理财的群，我发现，他们一只都在教我们理财的重要性，大家还在学的那么起劲，我再管网几天，看看又没有干货，有没有的话就退出吧。今天看完了爬虫的一段教程，我觉讲的很有条理，明天继续。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我不知道为什么要写这个东西，只是闲着无聊，有不想动笔写日记，反正也没有喜欢的本子，就把闲置的博客拿来用用吧，希望未来自己看到这个能够有所感触。&lt;/p&gt;
&lt;p&gt;我在从南京回来的路上听到一则鼓励写作的的录音，说写作并不用很好好的文笔只需要保持一直写就可以了。对于这个观点，我相信
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Datastruct</title>
    <link href="http://yoursite.com/2017/12/12/datastruct/"/>
    <id>http://yoursite.com/2017/12/12/datastruct/</id>
    <published>2017-12-12T02:51:57.000Z</published>
    <updated>2018-01-25T16:50:49.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性表"><a href="#线性表" class="headerlink" title="线性表"></a>线性表</h1><h2 id="顺序表示"><a href="#顺序表示" class="headerlink" title="顺序表示"></a>顺序表示</h2><p><strong>DestroyLis</strong>t是将线性表的所有内容释放。线性表有三个属性，分别是线性表的空间基址、当前内容的长度<strong>length</strong>以及分配的长度<strong>size</strong>。（这里分配的长度需要大于内容的长度）<strong>DestroyList</strong>是讲所有的属性抹去初始值。</p><p><strong>ClearList</strong>是将当前的内容长度置0。</p><p><strong>isEmpty</strong>则是通过内容长度来判断线性表是否为空。</p><p><strong>GetElem</strong>是将第i个元素取出来并返回。</p><p><strong>FindElem</strong>是将表中元素与查找元素相同的下标输出。</p><p>对应的还有插入、删除、访问、历遍等等。。。。。</p><h2 id="链式表示"><a href="#链式表示" class="headerlink" title="链式表示"></a>链式表示</h2><p>链式表示和顺序表示最大的不同就是，链式表示中逻辑位置相邻的数据他们需要物理位置也相邻。插入和删除不需要再将大量的数据做移动，减少算法的复杂度。</p><p>定义单链表时会有两个属性，1.数据，2.下一个节点的指针。</p><p>作业，编写代码<strong>实现链表反转和实现一元多项式的相加</strong>。</p><p>12/12/2017 11:46:54 AM </p><hr>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性表&quot;&gt;&lt;a href=&quot;#线性表&quot; class=&quot;headerlink&quot; title=&quot;线性表&quot;&gt;&lt;/a&gt;线性表&lt;/h1&gt;&lt;h2 id=&quot;顺序表示&quot;&gt;&lt;a href=&quot;#顺序表示&quot; class=&quot;headerlink&quot; title=&quot;顺序表示&quot;&gt;&lt;/a&gt;顺序表
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>考研前的课程复习安排</title>
    <link href="http://yoursite.com/2017/11/27/%E8%80%83%E7%A0%94%E5%89%8D%E7%9A%84%E8%AF%BE%E7%A8%8B%E5%A4%8D%E4%B9%A0%E5%AE%89%E6%8E%92/"/>
    <id>http://yoursite.com/2017/11/27/考研前的课程复习安排/</id>
    <published>2017-11-27T11:37:16.000Z</published>
    <updated>2018-01-25T16:51:34.418Z</updated>
    
    <content type="html"><![CDATA[<p>这学期可能是我大学期间的转折点，也将可能会是我大学阶段面对考试最困难的一个学期，因为我大三了，而且面临考验抉择，所以这个学期志在必得（一门都不许挂），下面是我最低限度的复习计划，针对每天整理出最少的复习量。</p><p><img src="https://i.imgur.com/nQ5pPSQ.jpg" alt=""></p><p>这学期的课程分为6门：通原，通线相对可以接受，交换，信息论，单片机，电磁场很难。</p><p>现在是第13周，距离考试还有4周不到的时间，我暂且粗略的估计是20天，还留七八天用来巩固。</p><h1 id="通原"><a href="#通原" class="headerlink" title="通原"></a>通原</h1><p>一共13章，一天一章，从明天开始。</p><h1 id="通线"><a href="#通线" class="headerlink" title="通线"></a>通线</h1><p>一共10章，一天一章，从明天开始。</p><h1 id="电磁场"><a href="#电磁场" class="headerlink" title="电磁场"></a>电磁场</h1><p>一共8章，一天一章，从明天开始。</p><h1 id="交换机"><a href="#交换机" class="headerlink" title="交换机"></a>交换机</h1><p>一共11章，一天一章，从12月6日开始。</p><h1 id="单片机"><a href="#单片机" class="headerlink" title="单片机"></a>单片机</h1><p>一共11章，一天一章，从12月7日开始。</p><p>#信息论 #<br>一共7章，一天一章，从12月11日开始。</p><p>这些课程除外还有一节毛概，一节通创，一节就业指导，以及一节实验。</p><p>综合起来将会有10门功课，加油吧，就这么多了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这学期可能是我大学期间的转折点，也将可能会是我大学阶段面对考试最困难的一个学期，因为我大三了，而且面临考验抉择，所以这个学期志在必得（一门都不许挂），下面是我最低限度的复习计划，针对每天整理出最少的复习量。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://i.imgur.
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/11/23/hello-world/"/>
    <id>http://yoursite.com/2017/11/23/hello-world/</id>
    <published>2017-11-23T15:47:33.091Z</published>
    <updated>2017-11-23T15:47:33.091Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
